{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZgKHfkBwh2LA"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLyS0rZh9Ql"
      },
      "source": [
        "Goal\n",
        "==\n",
        "We are about to train a *sequence-to-sequence model* to predict a paragraph of Gustave Flaubert's *Madame Bovary* given the preceding paragraph.\n",
        "The model (at least in its first version) does not use words as units of text but characters.\n",
        "\n",
        "*   The encoder part, based on a bidirectional LSTM, reads an input paragraph and turns it into a set of tensors that serves as initial state for the decoder part.\n",
        "*   The decoder part is based on an (unidirectional) LSTM. The state of the LSTM is used to compute a probability distribution over the alphabet (including space and punctuation marks) and is updated each time a character is predicted by the LSTM reading this character's embedding.\n",
        "*   The goal is to get the best model. It is part of the job to define what this means. It is also part of the job to explain me how you get your best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWD-p9yiECs"
      },
      "source": [
        "Loading PyTorch is important.\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An16FNHuhZI1"
      },
      "source": [
        "# Imports PyTorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is there: https://www.gutenberg.org/cache/epub/14155/pg14155.txt\n",
        "\n",
        "We have to pre-process it a little bit in order to remove everything that is not part of the text and to split the actual text into paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_toy_dataset = False # If True, a toy dataset (see below) is used, instead of the \"real\" one."
      ],
      "metadata": {
        "id": "veOPiPCIlOOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this place we set `use_toy_dataset = False` because preliminary experimental results show that the model already exhibits good performance on smaller datasets (see block's model prediction results below), which suggests that our model design is capable of meeting the needs of the target task. However, due to the limitation of the dataset size, the model may not be able to fully learn more complex or rare patterns, so we can further optimize the model by using larger datasets."
      ],
      "metadata": {
        "id": "qKCROPmITWRQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VXDpa1tiSfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9c96be-8bd4-4384-de65-d678f813d195"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")\n",
        "filename = tmp[0]\n",
        "print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpetb4icyd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4hirpsiWX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69f2afc-26e2-4817-d60f-80553e097d14"
      },
      "source": [
        "# Prints the first 200 lines in the file with their line number.\n",
        "# This shows that we have a little bit of preprocessing to do in order to clean the data.\n",
        "with open(filename) as f:\n",
        "  for i in range(200):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ﻿The Project Gutenberg eBook of Madame Bovary\n",
            "[1]     \n",
            "[2] This ebook is for the use of anyone anywhere in the United States and\n",
            "[3] most other parts of the world at no cost and with almost no restrictions\n",
            "[4] whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "[5] of the Project Gutenberg License included with this ebook or online\n",
            "[6] at www.gutenberg.org. If you are not located in the United States,\n",
            "[7] you will have to check the laws of the country where you are located\n",
            "[8] before using this eBook.\n",
            "[9] \n",
            "[10] Title: Madame Bovary\n",
            "[11] \n",
            "[12] Author: Gustave Flaubert\n",
            "[13] \n",
            "[14] Release date: November 26, 2004 [eBook #14155]\n",
            "[15]                 Most recently updated: December 18, 2020\n",
            "[16] \n",
            "[17] Language: French\n",
            "[18] \n",
            "[19] Credits: Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[20] \n",
            "[21] \n",
            "[22] *** START OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY ***\n",
            "[23] \n",
            "[24] \n",
            "[25] \n",
            "[26] \n",
            "[27] Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[28] \n",
            "[29] \n",
            "[30] \n",
            "[31] \n",
            "[32] \n",
            "[33] Gustave Flaubert\n",
            "[34] MADAME BOVARY\n",
            "[35] \n",
            "[36] \n",
            "[37] (1857)\n",
            "[38] \n",
            "[39] \n",
            "[40] Table des matières\n",
            "[41] \n",
            "[42] PREMIÈRE PARTIE\n",
            "[43] I\n",
            "[44] II\n",
            "[45] III\n",
            "[46] IV\n",
            "[47] V\n",
            "[48] VI\n",
            "[49] VII\n",
            "[50] VIII\n",
            "[51] IX\n",
            "[52] DEUXIÈME PARTIE\n",
            "[53] I\n",
            "[54] II\n",
            "[55] III\n",
            "[56] IV\n",
            "[57] V\n",
            "[58] VI\n",
            "[59] VII\n",
            "[60] VIII\n",
            "[61] IX\n",
            "[62] X\n",
            "[63] XI\n",
            "[64] XII\n",
            "[65] XIII\n",
            "[66] XIV\n",
            "[67] XV\n",
            "[68] TROISIÈME PARTIE\n",
            "[69] I\n",
            "[70] II\n",
            "[71] III\n",
            "[72] IV\n",
            "[73] V\n",
            "[74] VI\n",
            "[75] VII\n",
            "[76] VIII\n",
            "[77] IX\n",
            "[78] X\n",
            "[79] XI\n",
            "[80] \n",
            "[81] \n",
            "[82] À Marie-Antoine-Jules Senard\n",
            "[83] \n",
            "[84] MEMBRE DU BARREAU DE PARIS EX-PRESIDENT DE L'ASSEMBLÉE NATIONALE\n",
            "[85] ET ANCIEN MINISTRE DE L'INTÉRIEUR\n",
            "[86] \n",
            "[87] Cher et illustre ami,\n",
            "[88] \n",
            "[89] Permettez-moi d'inscrire votre nom en tête de ce livre et au-\n",
            "[90] dessus même de sa dédicace; car c'est à vous, surtout, que j'en\n",
            "[91] dois la publication. En passant par votre magnifique plaidoirie,\n",
            "[92] mon oeuvre a acquis pour moi-même comme une autorité imprévue.\n",
            "[93] Acceptez donc ici l'hommage de ma gratitude, qui, si grande\n",
            "[94] qu'elle puisse être, ne sera jamais à la hauteur de votre\n",
            "[95] éloquence et de votre dévouement.\n",
            "[96] \n",
            "[97] GUSTAVE FLAUBERT\n",
            "[98] \n",
            "[99] Paris, 12 avril 1857\n",
            "[100] \n",
            "[101] \n",
            "[102] À Louis Bouilhet\n",
            "[103] \n",
            "[104] \n",
            "[105] PREMIÈRE PARTIE\n",
            "[106] \n",
            "[107] \n",
            "[108] I\n",
            "[109] \n",
            "[110] Nous étions à l'Étude, quand le Proviseur entra, suivi d'un\n",
            "[111] nouveau habillé en bourgeois et d'un garçon de classe qui portait\n",
            "[112] un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se\n",
            "[113] leva comme surpris dans son travail.\n",
            "[114] \n",
            "[115] Le Proviseur nous fit signe de nous rasseoir; puis, se tournant\n",
            "[116] vers le maître d'études:\n",
            "[117] \n",
            "[118] -- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je\n",
            "[119] vous recommande, il entre en cinquième. Si son travail et sa\n",
            "[120] conduite sont méritoires, il passera dans les grands, où l'appelle\n",
            "[121] son âge.\n",
            "[122] \n",
            "[123] Resté dans l'angle, derrière la porte, si bien qu'on l'apercevait\n",
            "[124] à peine, le nouveau était un gars de la campagne, d'une quinzaine\n",
            "[125] d'années environ, et plus haut de taille qu'aucun de nous tous. Il\n",
            "[126] avait les cheveux coupés droit sur le front, comme un chantre de\n",
            "[127] village, l'air raisonnable et fort embarrassé. Quoiqu'il ne fût\n",
            "[128] pas large des épaules, son habit-veste de drap vert à boutons\n",
            "[129] noirs devait le gêner aux entournures et laissait voir, par la\n",
            "[130] fente des parements, des poignets rouges habitués à être nus. Ses\n",
            "[131] jambes, en bas bleus, sortaient d'un pantalon jaunâtre très tiré\n",
            "[132] par les bretelles. Il était chaussé de souliers forts, mal cirés,\n",
            "[133] garnis de clous.\n",
            "[134] \n",
            "[135] On commença la récitation des leçons. Il les écouta de toutes ses\n",
            "[136] oreilles, attentif comme au sermon, n'osant même croiser les\n",
            "[137] cuisses, ni s'appuyer sur le coude, et, à deux heures, quand la\n",
            "[138] cloche sonna, le maître d'études fut obligé de l'avertir, pour\n",
            "[139] qu'il se mît avec nous dans les rangs.\n",
            "[140] \n",
            "[141] Nous avions l'habitude, en entrant en classe, de jeter nos\n",
            "[142] casquettes par terre, afin d'avoir ensuite nos mains plus libres;\n",
            "[143] il fallait, dès le seuil de la porte, les lancer sous le banc, de\n",
            "[144] façon à frapper contre la muraille en faisant beaucoup de\n",
            "[145] poussière; c'était là le genre.\n",
            "[146] \n",
            "[147] Mais, soit qu'il n'eût pas remarqué cette manoeuvre ou qu'il n'eut\n",
            "[148] osé s'y soumettre, la prière était finie que le nouveau tenait\n",
            "[149] encore sa casquette sur ses deux genoux. C'était une de ces\n",
            "[150] coiffures d'ordre composite, où l'on retrouve les éléments du\n",
            "[151] bonnet à poil, du chapska, du chapeau rond, de la casquette de\n",
            "[152] loutre et du bonnet de coton, une de ces pauvres choses, enfin,\n",
            "[153] dont la laideur muette a des profondeurs d'expression comme le\n",
            "[154] visage d'un imbécile. Ovoïde et renflée de baleines, elle\n",
            "[155] commençait par trois boudins circulaires; puis s'alternaient,\n",
            "[156] séparés par une bande rouge, des losanges de velours et de poils\n",
            "[157] de lapin; venait ensuite une façon de sac qui se terminait par un\n",
            "[158] polygone cartonné, couvert d'une broderie en soutache compliquée,\n",
            "[159] et d'où pendait, au bout d'un long cordon trop mince, un petit\n",
            "[160] croisillon de fils d'or, en manière de gland. Elle était neuve; la\n",
            "[161] visière brillait.\n",
            "[162] \n",
            "[163] -- Levez-vous, dit le professeur.\n",
            "[164] \n",
            "[165] Il se leva; sa casquette tomba. Toute la classe se mit à rire.\n",
            "[166] \n",
            "[167] Il se baissa pour la reprendre. Un voisin la fit tomber d'un coup\n",
            "[168] de coude, il la ramassa encore une fois.\n",
            "[169] \n",
            "[170] -- Débarrassez-vous donc de votre casque, dit le professeur, qui\n",
            "[171] était un homme d'esprit.\n",
            "[172] \n",
            "[173] Il y eut un rire éclatant des écoliers qui décontenança le pauvre\n",
            "[174] garçon, si bien qu'il ne savait s'il fallait garder sa casquette à\n",
            "[175] la main, la laisser par terre ou la mettre sur sa tête. Il se\n",
            "[176] rassit et la posa sur ses genoux.\n",
            "[177] \n",
            "[178] -- Levez-vous, reprit le professeur, et dites-moi votre nom.\n",
            "[179] \n",
            "[180] Le nouveau articula, d'une voix bredouillante, un nom\n",
            "[181] inintelligible.\n",
            "[182] \n",
            "[183] -- Répétez!\n",
            "[184] \n",
            "[185] Le même bredouillement de syllabes se fit entendre, couvert par\n",
            "[186] les huées de la classe.\n",
            "[187] \n",
            "[188] -- Plus haut! cria le maître, plus haut!\n",
            "[189] \n",
            "[190] Le nouveau, prenant alors une résolution extrême, ouvrit une\n",
            "[191] bouche démesurée et lança à pleins poumons, comme pour appeler\n",
            "[192] quelqu'un, ce mot: Charbovari.\n",
            "[193] \n",
            "[194] Ce fut un vacarme qui s'élança d'un bond, monta en crescendo, avec\n",
            "[195] des éclats de voix aigus (on hurlait, on aboyait, on trépignait,\n",
            "[196] on répétait: Charbovari! Charbovari!), puis qui roula en notes\n",
            "[197] isolées, se calmant à grand-peine, et parfois qui reprenait tout à\n",
            "[198] coup sur la ligne d'un banc où saillissait encore çà et là, comme\n",
            "[199] un pétard mal éteint, quelque rire étouffé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctl4Z9Gti-6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792dc23e-5a7c-42dc-9b91-0dfb5a2f6fda"
      },
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "\n",
        "with open(filename) as f:\n",
        "  # We want to skip everything before the actual text of the novel.\n",
        "  # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "  # The following lines discard everything up to this second occurence (included).\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"PREMIÈRE PARTIE\"): skip -= 1;\n",
        "\n",
        "  paragraphs = [] # Note that each dialog line will be considered a separate paragraph.\n",
        "  paragraph_buffer = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(\"END OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY\" in line): break # End of the actual text.\n",
        "\n",
        "    if(line == \"\"): # We've reached the end of a paragraph.\n",
        "      if(len(paragraph_buffer) > 0):\n",
        "        paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs.append(paragraph)\n",
        "        paragraph_buffer = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')): paragraph_buffer.append(' ') # Adds a space between consecutive lines except when the first one ends with \"-\" (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next, we do not want to join the two lines with a space).\n",
        "    paragraph_buffer.append(line)\n",
        "\n",
        "print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "for i in range(3): print(paragraphs[i], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2995 paragraphs read.\n",
            "Nous étions à l'Étude, quand le Proviseur entra, suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se leva comme surpris dans son travail.\n",
            "Le Proviseur nous fit signe de nous rasseoir; puis, se tournant vers le maître d'études:\n",
            "-- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je vous recommande, il entre en cinquième. Si son travail et sa conduite sont méritoires, il passera dans les grands, où l'appelle son âge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a toy dataset on which your model, if correctly implemented, should be able to learn more easily."
      ],
      "metadata": {
        "id": "QZttAP5uj6y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_toy_dataset):\n",
        "  paragraphs = []\n",
        "\n",
        "  import random, string\n",
        "  characters = list(string.ascii_lowercase + string.ascii_lowercase.upper() + \"_-/\\'[]()\")\n",
        "  random.shuffle(characters)\n",
        "  k = random.randint(1, 10)\n",
        "  a = \"a\"\n",
        "  paragraph = (a * k)\n",
        "  for _ in range(100):\n",
        "    random.shuffle(characters)\n",
        "    for a in characters:\n",
        "      k = random.randint(1, 16)\n",
        "      paragraph += f\"? Now, please write {k} {a}.{EOP}\"\n",
        "      paragraphs.append(paragraph)\n",
        "      paragraph = (a * k)\n",
        "  print(f\"{len(paragraphs)} paragraphs generated.\")\n",
        "\n",
        "  print(paragraphs[:10])"
      ],
      "metadata": {
        "id": "28V5KvT6jUFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are using a larger dataset, we also split the dataset at `rate=0.8` into `80% training set` and `20% dev set`, where the training set is used to learn the parameters of the model and the dev set is used to evaluate the generalization ability of the model. The advantage of this is that the dev set is data that the model has not seen during the training process, and by monitoring the model's performance on the dev set, the occurrence of **overfitting** can be detected and the training can be terminated in a timely manner through **early stopping**. In addition, the dev set can be used as a basis for **hyper-parameter tuning**, e.g., adjusting the learning rate, batch size, etc."
      ],
      "metadata": {
        "id": "0IGcBm8vbI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate datasets into train and dev\n",
        "\n",
        "n = len(paragraphs)\n",
        "rate = 0.8\n",
        "paragraphs, paragraphs_dev = paragraphs[:int(n*rate)], paragraphs[int(n*rate):]\n",
        "print(f\"{len(paragraphs)} paragraphs in the training set.\")\n",
        "print(f\"{len(paragraphs_dev)} paragraphs in the development set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S12LP6BuZlL-",
        "outputId": "514ff743-5064-484a-bfd2-2e5ff456cc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2396 paragraphs in the training set.\n",
            "599 paragraphs in the development set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAn1SqQE1Hr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c37e71-49b5-4c27-ea85-7240466af68c"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "char_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for char in paragraph: char_counts[char] += 1\n",
        "\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\")\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94 different characters found in the dataset.\n",
            "[(' ', 93470), ('e', 65537), ('a', 38453), ('s', 36953), ('t', 33017), ('i', 32454), ('n', 29943), ('r', 29188), ('l', 28739), ('u', 28107), ('o', 23359), ('d', 16560), ('c', 12613), ('m', 12507), ('p', 11738), (',', 10667), ('v', 7238), ('é', 7020), (\"'\", 6278), ('.', 5130), ('q', 4724), ('b', 4717), ('f', 4583), ('h', 4516), ('g', 4036), ('-', 3519), ('\\n', 2396), ('à', 2360), ('x', 1731), ('j', 1491), ('y', 1400), ('è', 1383), ('E', 1239), (';', 1204), ('!', 1188), ('ê', 1011), ('L', 835), ('C', 796), ('M', 630), ('I', 590), ('z', 575), ('?', 455), ('A', 451), ('ç', 391), (':', 378), ('B', 356), ('â', 345), ('P', 320), ('î', 283), ('R', 275), ('D', 269), ('S', 262), ('ù', 253), ('ô', 249), ('O', 243), ('Q', 209), ('û', 202), ('H', 198), ('J', 182), ('V', 158), ('T', 157), ('N', 132), ('U', 99), ('«', 99), ('»', 91), ('À', 72), ('Y', 69), ('F', 60), ('G', 52), ('(', 44), (')', 44), ('_', 40), ('ï', 33), ('É', 21), ('1', 16), ('k', 15), ('9', 8), ('3', 8), ('2', 7), ('ë', 7), ('Ç', 7), ('X', 6), ('6', 6), ('5', 5), ('4', 5), ('8', 4), ('Ê', 4), ('ü', 4), ('W', 3), ('7', 3), ('w', 2), ('°', 2), ('0', 1), ('Î', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF0YFZTxD_A0"
      },
      "source": [
        "# char2id\n",
        "char_vocabulary = {char : i for i, (char, _) in enumerate(char_counts.items())}\n",
        "# id2char\n",
        "id_to_char = list(char_vocabulary.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71VlK5e3Gg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e84a36-9cbb-425d-c4b9-e713a1c8d737"
      },
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "padding_id = EOP_id + 1 # Pseudo-id for the \"padding symbol\" (which in fact does not exist)\n",
        "\n",
        "print(char_vocabulary)\n",
        "print(id_to_char)\n",
        "print(f\"EOP_id = {EOP_id}\")\n",
        "\n",
        "def test_consistent(char_vocabulary, id_to_char):\n",
        "    for char, id in char_vocabulary.items():\n",
        "        assert char == id_to_char[id]\n",
        "    for id, char in enumerate(id_to_char):\n",
        "        assert id == char_vocabulary[char]\n",
        "    print(\"Test successfully, the implementations of 'char_vocabulary' and 'id_to_char' are consistent.\")\n",
        "\n",
        "test_consistent(char_vocabulary, id_to_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'N': 0, 'o': 1, 'u': 2, 's': 3, ' ': 4, 'é': 5, 't': 6, 'i': 7, 'n': 8, 'à': 9, 'l': 10, \"'\": 11, 'É': 12, 'd': 13, 'e': 14, ',': 15, 'q': 16, 'a': 17, 'P': 18, 'r': 19, 'v': 20, 'h': 21, 'b': 22, 'g': 23, 'ç': 24, 'c': 25, 'p': 26, '.': 27, 'C': 28, 'x': 29, 'm': 30, 'è': 31, '\\n': 32, 'L': 33, 'f': 34, ';': 35, 'î': 36, ':': 37, '-': 38, 'M': 39, 'R': 40, 'j': 41, 'S': 42, 'ù': 43, 'â': 44, 'z': 45, 'I': 46, 'Q': 47, 'û': 48, 'ê': 49, 'O': 50, 'y': 51, 'k': 52, 'ï': 53, 'E': 54, 'T': 55, 'U': 56, 'D': 57, '!': 58, '(': 59, ')': 60, 'B': 61, '?': 62, '_': 63, 'G': 64, '1': 65, '8': 66, '2': 67, 'A': 68, 'À': 69, 'H': 70, 'ô': 71, 'V': 72, 'Ê': 73, '«': 74, '»': 75, 'Y': 76, 'F': 77, 'J': 78, 'ë': 79, 'W': 80, 'X': 81, '0': 82, '5': 83, '7': 84, '9': 85, '6': 86, '3': 87, 'w': 88, 'Î': 89, 'Ç': 90, '4': 91, 'ü': 92, '°': 93}\n",
            "['N', 'o', 'u', 's', ' ', 'é', 't', 'i', 'n', 'à', 'l', \"'\", 'É', 'd', 'e', ',', 'q', 'a', 'P', 'r', 'v', 'h', 'b', 'g', 'ç', 'c', 'p', '.', 'C', 'x', 'm', 'è', '\\n', 'L', 'f', ';', 'î', ':', '-', 'M', 'R', 'j', 'S', 'ù', 'â', 'z', 'I', 'Q', 'û', 'ê', 'O', 'y', 'k', 'ï', 'E', 'T', 'U', 'D', '!', '(', ')', 'B', '?', '_', 'G', '1', '8', '2', 'A', 'À', 'H', 'ô', 'V', 'Ê', '«', '»', 'Y', 'F', 'J', 'ë', 'W', 'X', '0', '5', '7', '9', '6', '3', 'w', 'Î', 'Ç', '4', 'ü', '°']\n",
            "EOP_id = 32\n",
            "Test successfully, the implementations of 'char_vocabulary' and 'id_to_char' are consistent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc3Gst0zhQv"
      },
      "source": [
        "# Turns a list of lists of ids into a list of strings.\n",
        "# Do not forget that an occurrence of EOP means that the paragraph ends here.\n",
        "# ids: list[int]\n",
        "def ids_to_texts(ids):\n",
        "  results = []\n",
        "  for id in ids:\n",
        "    chars = []\n",
        "    for item in id:\n",
        "      if item == EOP_id:\n",
        "        break\n",
        "      chars.append(id_to_char[item])\n",
        "    results.append(''.join(chars))\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAhzb4_3RTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ace0d77-27c7-421f-c8c4-98724d1a3ebc"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[61, 1, 8, 41, 1, 2, 19, 27], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnT7T-yHEPCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb648825-162c-4381-e939-72d2b1531707"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "ids[0].extend([EOP_id, padding_id, padding_id]) # With the end-of-paragraph token id and additional (padding-like) stuff for the first string.\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\") # If you have a problem here, remember that EOP indicates the end of the text (this might be related to your problem)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[61, 1, 8, 41, 1, 2, 19, 27, 32, 33, 33], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH4g-Fkkrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75195919-2993-402c-ecc0-fe62c620a136"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "# A training instance is composed of a pair of consecutive paragraphs. The goal will be to predict the second given the first.\n",
        "# (Possible improvement: As is, ends of chapter are completely ignored: the last paragraph of a chapter and the first of the following chapter form a training instance. We might want to predict the end of the chapter instead, or simply remove these pairs from the dataset.)\n",
        "class BatchGenerator:\n",
        "  # paragraphs: list[str]\n",
        "  # char_vocabulary: dict[str, int]\n",
        "  def __init__(self, paragraphs, char_vocabulary):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.char_vocabulary = char_vocabulary # Dictionary\n",
        "    self.padding_idx = len(char_vocabulary)\n",
        "\n",
        "  # Returns the number of training instances (i.e. of pairs of consecutive paragraphs).\n",
        "  def length(self):\n",
        "    return (len(self.paragraphs) - 1)\n",
        "\n",
        "  # Returns a random training batch (composed of pairs of consecutive paragraphs).\n",
        "  # batch_size: int\n",
        "  # subset: int | None; if an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def get_batch(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "    paragraph_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some paragraph ids.\n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids)\n",
        "\n",
        "  # paragraph_ids: sequence of int·s\n",
        "  def _ids_to_batch(self, paragraph_ids):\n",
        "    firsts = [] # list[list[int]]; first paragraph of each pair\n",
        "    seconds = [] # list[list[int]], second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      firsts.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id]])\n",
        "      seconds.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id + 1]])\n",
        "\n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long) # Conversion to a tensor\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long) # Conversion to a tensor\n",
        "\n",
        "    return (firsts, seconds)\n",
        "\n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # Works \"in place\".\n",
        "  # sequences: list[list[int]]\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "\n",
        "  # Returns a generator of training batches for a full epoch. (Note that this function is not used in the training loop implemented below. `get_batch` is used instead.)\n",
        "  # batch_size: int\n",
        "  # subset: int | None; if an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def all_batches(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "      i += batch_size\n",
        "\n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "\n",
        "  # Turns a list of arbitrary paragraphs into a prediction batch.\n",
        "  # paragraphs: list[str]\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = [] # list[list[int]]\n",
        "    for paragraph in paragraphs:\n",
        "        tmp = [] # list[int]\n",
        "        for char in paragraph:\n",
        "          # Unknown characters are ignored (removed).\n",
        "          if(char in self.char_vocabulary): tmp.append(self.char_vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "\n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, char_vocabulary=char_vocabulary)\n",
        "dev_batch_generator = BatchGenerator(paragraphs=paragraphs_dev, char_vocabulary=char_vocabulary)\n",
        "print(f\" {batch_generator.length()} examples for train\")\n",
        "print(f\" {dev_batch_generator.length()} examples for dev\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2395 examples for train\n",
            " 598 examples for dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see at the end of the previous code block that `batch_generator` is generated based on the training set (paragraphs_train) for training the model and `dev_batch_generator` is generated based on the dev set (paragraphs_dev) for model evaluation. Such operations provide the basis for later model optimization."
      ],
      "metadata": {
        "id": "M7Z4LBgdhfuC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YSugGI7z3JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4567883-8969-4006-fc92-07446ae5a926"
      },
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3)\n",
        "print(ids_to_texts(firsts))\n",
        "print(ids_to_texts(seconds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"-- Une chose... grave, sérieuse. Eh! non, d'ailleurs, vous ne partirez pas, c'est impossible! Si vous saviez... Écoutez-moi... Vous ne m'avez donc pas compris? vous n'avez pas deviné?...\", '-- Tantôt, par exemple, quand je suis venu chez vous...', \"Justin ne répondait pas. L'apothicaire continuait:\"]\n",
            "['-- Cependant vous parlez bien, dit Emma.', '«À M. Bizet, de Quincampoix.»', \"-- Qui t'a prié de venir? Tu importunes toujours monsieur et madame! Les mercredis, d'ailleurs, ta présence m'est plus indispensable. Il y a maintenant vingt personnes à la maison. J'ai tout quitté à cause de l'intérêt que je te porte. Allons, va-t'en! cours! attends-moi, et surveille les bocaux!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "==\n",
        "For this model, we will not define a `forward` method, but two methods: `trainingLogits` and `predictionStrings`.\n",
        "\n",
        "*    `trainingLogits` is used at training time, when each batch is split in two parts: input paragraphs and output paragraphs. This function outputs, for each output paragraph of the batch, a log-probability distribution (i.e. a vector of \"logits\") before each token and after the last one. These distributions depend on the encoding of the corresponding input paragraph. They will then be used to compute a loss value.\n",
        "*    `predictionStrings` is used at prediction time, when each batch is only composed of input paragraphs. This function outputs, for each input paragraph, a string obtained by decoding the encoding of the paragraph and its probability according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfRCXQOOm8X"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  # size_vocabulary: int; does not include a padding character, but does include the end-of-paragraph one.\n",
        "  # EOP_id: int\n",
        "  # embedding_dim: int\n",
        "  # lstm_hidden_size: int\n",
        "  # lstm_layers: int\n",
        "  # device: str\n",
        "  def __init__(self, size_vocabulary, EOP_id, embedding_dim, lstm_hidden_size, lstm_layers, device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.EOP_id = EOP_id # At prediction time, this index is used to stop the generation at the end of the paragraph.\n",
        "\n",
        "    #  (i) an embedding layer 'self.char_embeddings' with 'torch.nn.Embedding' for the characters, including an padding embedding;\n",
        "    #  (ii) a bidirectional LSTM 'self.encoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iii) a unidirectional LSTM 'self.decoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iv) a network 'self.decoder_initialiser' meant to turn the final hidden and cell states of the encoder into the initial hidden and cell states of the decoder;\n",
        "    #  (v) a network 'self.distribution_nn' meant to turn the hidden state of the decoder at each step into the logits of a probability distribution over the vocabulary. The logits of a probability distribution are simply the log-probabilities (you might want to use torch.nn.LogSoftmax).\n",
        "    self.char_embeddings = torch.nn.Embedding(size_vocabulary+1, embedding_dim, padding_idx=size_vocabulary).to(device) # size_vocabulary +1 for the pad token\n",
        "    self.encoder_lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_size, lstm_layers, batch_first=True, bidirectional=True).to(device)\n",
        "    self.decoder_lstm = torch.nn.LSTM(embedding_dim, lstm_hidden_size, lstm_layers, batch_first=True).to(device)\n",
        "    self.decoder_initialiser = torch.nn.Sequential(\n",
        "        torch.nn.Linear(lstm_hidden_size * 2, lstm_hidden_size),\n",
        "        torch.nn.Tanh()\n",
        "    ).to(device)\n",
        "    self.distribution_nn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(lstm_hidden_size, size_vocabulary),\n",
        "        torch.nn.LogSoftmax(dim=-1)\n",
        "    ).to(device)\n",
        "\n",
        "  # This function encodes the input paragraphs and turns them into initial states for the decoder. It is used both at training and prediction time.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  def initStates(self, in_paragraphs):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.char_embeddings(in_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False) # Enables the biLSTM to ignore padding elements.\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings) # 'h_n' and 'c_n' are both of shape (num_layers * 2, batch_size, hidden_size)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "\n",
        "    # The shape of the two tensors of the following pair : (num_layers, batch_size, lstm_hidden_size)\n",
        "    return (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n))\n",
        "\n",
        "  # Training time: This function outputs the logits for each time step.\n",
        "  # Because at training time, the output paragraph is known, there is no need to generate anything sequentially — all positions can be processed at the same time. In fact, there is a loop hidden in the call to the decoder LSTM, but you should not write any explicit loop here.\n",
        "\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # 'out_paragraphs' is a matrix (batch size, max out length) of character ids (Integer) at training time. This might include EoS and padding tokens; the corresponding distributions are meaningless/useless and will have to be excluded when computing the loss.\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def trainingLogits(self, in_paragraphs, out_paragraphs):\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "    # Feed a packed sequence to the decoder (use 'torch.nn.utils.rnn.pack_padded_sequence' and 'torch.nn.utils.rnn.pad_packed_sequence').\n",
        "    # get first n-1 characters\n",
        "    out_paragraphs = out_paragraphs[:, :-1]\n",
        "    # embed out_paragraphs\n",
        "    out_char_embeddings = self.char_embeddings(out_paragraphs) # (batch_size, max_length, embedding_dim)\n",
        "    out_lengths = (out_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # (batch_size)\n",
        "    # enable the uniLSTM to ignore padding elements\n",
        "    out_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=out_char_embeddings, lengths=out_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    # decode the out_char_embbeddings with LSTM\n",
        "    output, _ = self.decoder_lstm(out_char_embeddings, decoder_init_states) # output: (batch_size, num_layers, hidden_size)\n",
        "    # pad a packed batch of variable length sequences\n",
        "    seq_unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "    # comput logits using a feedforward layer\n",
        "    logits = self.distribution_nn(seq_unpacked) # (batch_size, max_length, size_vocabulary)\n",
        "    # get first char logits of h_t\n",
        "    first_char_logits = self.distribution_nn(decoder_init_states[0][-1]).unsqueeze(1)\n",
        "    # combine the first char logits and the first n-1 char logits\n",
        "    logits = torch.cat([first_char_logits, logits], dim=1)\n",
        "    return logits\n",
        "\n",
        "  # Prediction time: This function generates a text up to 'max_predicted_char' character long for each paragraph in the batch, then returns the paragraphs and their log-probabilities.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  def predictionStrings(self, in_paragraphs, max_predicted_char=1000):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Decode 'decoder_init_states' into a matrix a character ids (on line per input paragraph in the batch) and then convert it to strings of actual characters.\n",
        "    # To work with probability distributions, you may use \"torch.distributions.Categorical\", but not necessarily.\n",
        "    # To compute the log-probabilities of the paragraphs generated, at some point you probably need something like `logits.gather(1, next_characters.unsqueeze(1)).squeeze(1)`, evaluating as a float tensor of shape (batch_size).\n",
        "\n",
        "    # get first char logits of h_t\n",
        "    first_char_logits = self.distribution_nn(decoder_init_states[0][-1]) # (batch_size, size_vocabulary)\n",
        "    # get first char distribution\n",
        "    first_char_distribution = torch.distributions.Categorical(logits=first_char_logits)\n",
        "    # sample first char id\n",
        "    texts_ids = first_char_distribution.sample().unsqueeze(1) # (batch_size, 1 or seq_length)\n",
        "    # get first char probability\n",
        "    logprobs = first_char_distribution.log_prob(texts_ids.squeeze())\n",
        "\n",
        "    temperature = 0.5\n",
        "    for i in range(max_predicted_char):\n",
        "        # embed input\n",
        "        in_embeddings = self.char_embeddings(texts_ids) # (batch_size, seq_length, embedding_dim)\n",
        "        # LSTM Decoder\n",
        "        output, _ = self.decoder_lstm(in_embeddings, decoder_init_states) # (batch_size, seq_length, hidden_size)\n",
        "        # comput logits using a feedforward layer\n",
        "        logits = self.distribution_nn(output[:, -1]).squeeze(1) # (batch_size, size_vocabulary)\n",
        "        # adjust temperature parameter for diversity generation\n",
        "        logits = logits * (1 / temperature)\n",
        "        # sample next character using probabilities\n",
        "        char_distribution = torch.distributions.Categorical(logits=logits)\n",
        "        # sample\n",
        "        next_character_id = char_distribution.sample()  # (batch_size, 1)\n",
        "        # update log-probs\n",
        "        logprobs += char_distribution.log_prob(next_character_id)\n",
        "\n",
        "        # stop if all sequences have ended\n",
        "        if all(char_id == self.EOP_id for char_id in next_character_id.tolist()):\n",
        "            break\n",
        "        # concatenate texts\n",
        "        texts_ids = torch.cat([texts_ids, next_character_id.unsqueeze(1)], dim=1)\n",
        "    # ids to texts\n",
        "    texts = ids_to_texts(texts_ids.tolist())\n",
        "\n",
        "    return texts, logprobs.to('cpu') # List[str], Tensor[float]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TXsvizgogZ"
      },
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=19, lstm_hidden_size=13, lstm_layers=7, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sp6c9Bdch3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48f59c2-4d25-4551-c290-d2a3716bd616"
      },
      "source": [
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device) # A batch that contains only one sentence with no padding.\n",
        "print(in_paragraphs)\n",
        "\n",
        "out_paragraphs = in_paragraphs\n",
        "logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "print(f\"logits:\\n{logits}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]], device='cuda:0')\n",
            "logits:\n",
            "tensor([[[-4.3008, -4.2767, -4.7207, -4.7277, -4.3843, -4.7423, -4.7044,\n",
            "          -4.7589, -4.6740, -4.3111, -4.3857, -4.6392, -4.6980, -4.7543,\n",
            "          -4.7473, -4.8411, -4.6294, -4.8713, -4.5355, -4.5264, -4.4939,\n",
            "          -4.2835, -4.3433, -4.4460, -4.6184, -4.5391, -4.3350, -4.6762,\n",
            "          -4.7373, -4.5289, -4.2737, -4.3374, -4.8358, -4.4482, -4.7534,\n",
            "          -4.7718, -4.2611, -4.6853, -4.7855, -4.4814, -4.2451, -4.7244,\n",
            "          -4.7501, -4.3703, -4.5005, -4.5582, -4.6608, -4.6974, -4.8005,\n",
            "          -4.4002, -4.4440, -4.6665, -4.4860, -4.5261, -4.5608, -4.3815,\n",
            "          -4.4183, -4.6138, -4.6199, -4.7757, -4.5441, -4.4257, -4.6187,\n",
            "          -4.4778, -4.6711, -4.5264, -4.4047, -4.3118, -4.5501, -4.6363,\n",
            "          -4.5149, -4.7297, -4.5829, -4.4764, -4.8497, -4.5448, -4.7044,\n",
            "          -4.3707, -4.2038, -4.4320, -4.2087, -4.4585, -4.7923, -4.6501,\n",
            "          -4.5405, -4.1758, -4.6614, -4.6667, -4.7192, -4.6135, -4.7259,\n",
            "          -4.4993, -4.4005, -4.7402],\n",
            "         [-4.3613, -4.3109, -4.7258, -4.6587, -4.3619, -4.6906, -4.6720,\n",
            "          -4.7225, -4.6864, -4.2215, -4.3947, -4.6360, -4.6526, -4.8049,\n",
            "          -4.7378, -4.7920, -4.6067, -4.8312, -4.6090, -4.5514, -4.4380,\n",
            "          -4.3308, -4.3367, -4.4441, -4.7077, -4.5068, -4.2381, -4.6406,\n",
            "          -4.6963, -4.4640, -4.2509, -4.4127, -4.7441, -4.4350, -4.7916,\n",
            "          -4.7731, -4.2867, -4.7127, -4.7409, -4.4984, -4.2829, -4.7215,\n",
            "          -4.6870, -4.4240, -4.4640, -4.5103, -4.6429, -4.7613, -4.7934,\n",
            "          -4.4316, -4.5614, -4.6611, -4.4893, -4.4717, -4.6550, -4.3533,\n",
            "          -4.4981, -4.6197, -4.5688, -4.7127, -4.5517, -4.4556, -4.6109,\n",
            "          -4.5563, -4.6088, -4.5699, -4.3886, -4.2921, -4.6083, -4.6671,\n",
            "          -4.5639, -4.6508, -4.5386, -4.5313, -4.8375, -4.5200, -4.6981,\n",
            "          -4.4045, -4.2639, -4.4288, -4.1511, -4.4766, -4.7723, -4.5873,\n",
            "          -4.5718, -4.2246, -4.6963, -4.7357, -4.7044, -4.5742, -4.7241,\n",
            "          -4.4339, -4.3818, -4.8337],\n",
            "         [-4.3870, -4.3270, -4.7249, -4.6323, -4.3665, -4.6648, -4.6590,\n",
            "          -4.7029, -4.6794, -4.1923, -4.4016, -4.6460, -4.6027, -4.8241,\n",
            "          -4.7459, -4.7803, -4.6069, -4.8218, -4.6371, -4.5657, -4.4177,\n",
            "          -4.3601, -4.3283, -4.4540, -4.7458, -4.5045, -4.2099, -4.6111,\n",
            "          -4.6909, -4.4390, -4.2450, -4.4405, -4.7026, -4.4262, -4.8230,\n",
            "          -4.7859, -4.2832, -4.7137, -4.7229, -4.4994, -4.2963, -4.7038,\n",
            "          -4.6626, -4.4529, -4.4673, -4.5037, -4.6630, -4.7874, -4.7878,\n",
            "          -4.4501, -4.6178, -4.6398, -4.5009, -4.4580, -4.6882, -4.3538,\n",
            "          -4.5425, -4.6138, -4.5478, -4.6846, -4.5396, -4.4700, -4.6083,\n",
            "          -4.5958, -4.5667, -4.6127, -4.3972, -4.2721, -4.6397, -4.6602,\n",
            "          -4.5803, -4.6260, -4.5091, -4.5462, -4.8405, -4.5142, -4.6991,\n",
            "          -4.4144, -4.2799, -4.4267, -4.1203, -4.4989, -4.7567, -4.5673,\n",
            "          -4.5686, -4.2193, -4.7008, -4.7677, -4.7080, -4.5560, -4.7307,\n",
            "          -4.3987, -4.3924, -4.8444],\n",
            "         [-4.3983, -4.3357, -4.7231, -4.6180, -4.3762, -4.6511, -4.6538,\n",
            "          -4.6905, -4.6696, -4.1789, -4.4098, -4.6518, -4.5662, -4.8307,\n",
            "          -4.7533, -4.7779, -4.6110, -4.8196, -4.6487, -4.5747, -4.4087,\n",
            "          -4.3802, -4.3261, -4.4573, -4.7645, -4.5008, -4.2005, -4.5935,\n",
            "          -4.6902, -4.4269, -4.2410, -4.4545, -4.6800, -4.4170, -4.8428,\n",
            "          -4.7957, -4.2767, -4.7085, -4.7167, -4.5017, -4.3057, -4.6889,\n",
            "          -4.6518, -4.4675, -4.4731, -4.5052, -4.6782, -4.7970, -4.7845,\n",
            "          -4.4567, -4.6484, -4.6237, -4.5082, -4.4516, -4.7058, -4.3552,\n",
            "          -4.5648, -4.6103, -4.5376, -4.6653, -4.5283, -4.4750, -4.6039,\n",
            "          -4.6180, -4.5367, -4.6397, -4.4067, -4.2616, -4.6555, -4.6566,\n",
            "          -4.5868, -4.6127, -4.4965, -4.5571, -4.8448, -4.5129, -4.7044,\n",
            "          -4.4181, -4.2898, -4.4277, -4.1044, -4.5132, -4.7473, -4.5561,\n",
            "          -4.5618, -4.2153, -4.7035, -4.7825, -4.7098, -4.5474, -4.7378,\n",
            "          -4.3837, -4.4014, -4.8430],\n",
            "         [-4.4030, -4.3405, -4.7209, -4.6103, -4.3854, -4.6430, -4.6519,\n",
            "          -4.6823, -4.6613, -4.1723, -4.4171, -4.6549, -4.5399, -4.8318,\n",
            "          -4.7597, -4.7793, -4.6163, -4.8192, -4.6529, -4.5804, -4.4047,\n",
            "          -4.3937, -4.3271, -4.4577, -4.7741, -4.4976, -4.1999, -4.5818,\n",
            "          -4.6912, -4.4216, -4.2388, -4.4613, -4.6685, -4.4087, -4.8567,\n",
            "          -4.8023, -4.2692, -4.7011, -4.7165, -4.5043, -4.3109, -4.6777,\n",
            "          -4.6478, -4.4754, -4.4795, -4.5097, -4.6899, -4.7991, -4.7826,\n",
            "          -4.4580, -4.6655, -4.6115, -4.5133, -4.4489, -4.7140, -4.3571,\n",
            "          -4.5773, -4.6088, -4.5317, -4.6536, -4.5188, -4.4767, -4.5993,\n",
            "          -4.6296, -4.5160, -4.6572, -4.4145, -4.2553, -4.6635, -4.6539,\n",
            "          -4.5896, -4.6058, -4.4906, -4.5648, -4.8488, -4.5126, -4.7098,\n",
            "          -4.4187, -4.2965, -4.4299, -4.0971, -4.5222, -4.7406, -4.5491,\n",
            "          -4.5561, -4.2118, -4.7038, -4.7895, -4.7097, -4.5429, -4.7451,\n",
            "          -4.3774, -4.4077, -4.8371]]], device='cuda:0',\n",
            "       grad_fn=<CatBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks that a tensor is a tensor of logits, i.e. that each vector is a vector of log-probabilities.\n",
        "def check_logits(logits):\n",
        "  # This function should return True iff `logits` is a tensor of logits (False otherwise).\n",
        "    return torch.all(torch.exp(logits) >= 0) and torch.all(torch.exp(logits) <= 1) and torch.all(torch.abs(torch.sum(torch.exp(logits), dim=-1) - 1) < 1e-5)\n",
        "\n",
        "print(f\"The following should be True: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.2], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.0], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.6, 2.0], [0.1, 0.2, 0.7]])))}\")\n",
        "print(f\"The following should be False: {check_logits(torch.log(torch.tensor([[0.5, 0.3, 0.2], [0.1, 0.2, 0.9]])))}\")"
      ],
      "metadata": {
        "id": "ww6UwsI2cuPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cf14f5-4483-406a-f1d1-fb0a3c4d965b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following should be True: True\n",
            "The following should be False: False\n",
            "The following should be False: False\n",
            "The following should be False: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The following should be True: {check_logits(logits)}\")"
      ],
      "metadata": {
        "id": "6bVNvxt5czmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8891e850-af6d-4e84-ca47-98e089f3225e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device) # A batch that contains two sentences with some padding (more than necessary).\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "print(logits)\n",
        "\n",
        "print(f\"The following should be True: {check_logits(logits)}\")"
      ],
      "metadata": {
        "id": "ftAttD70si-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e926f38a-5f46-45b1-bfb9-18c2da67adf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94],\n",
            "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 94, 94, 94, 94, 94]],\n",
            "       device='cuda:0')\n",
            "tensor([[[-4.3008, -4.2767, -4.7207,  ..., -4.4993, -4.4005, -4.7402],\n",
            "         [-4.3613, -4.3109, -4.7258,  ..., -4.4339, -4.3818, -4.8337],\n",
            "         [-4.3870, -4.3270, -4.7249,  ..., -4.3987, -4.3924, -4.8444],\n",
            "         ...,\n",
            "         [-4.3031, -4.2932, -4.6924,  ..., -4.4850, -4.3299, -4.7645],\n",
            "         [-4.3031, -4.2932, -4.6924,  ..., -4.4850, -4.3299, -4.7645],\n",
            "         [-4.3031, -4.2932, -4.6924,  ..., -4.4850, -4.3299, -4.7645]],\n",
            "\n",
            "        [[-4.2996, -4.2758, -4.7221,  ..., -4.4998, -4.4026, -4.7434],\n",
            "         [-4.3603, -4.3101, -4.7269,  ..., -4.4347, -4.3817, -4.8358],\n",
            "         [-4.3865, -4.3265, -4.7256,  ..., -4.3993, -4.3921, -4.8457],\n",
            "         ...,\n",
            "         [-4.4058, -4.3466, -4.7160,  ..., -4.3731, -4.4186, -4.8181],\n",
            "         [-4.4059, -4.3471, -4.7156,  ..., -4.3731, -4.4197, -4.8159],\n",
            "         [-4.4059, -4.3475, -4.7154,  ..., -4.3731, -4.4204, -4.8144]]],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "The following should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPGm_Duq_QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616da239-092e-4c46-8fe7-43368df6a885"
      },
      "source": [
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2)\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['xbOXWaëjë6o;kTÊat', 'H0tüpd8-êù1lAbJRM'],\n",
              " tensor([-77.7743, -76.7712], grad_fn=<ToCopyBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During model training, we added **BLEU** metrics to improve the diversity of model evaluations. In text generation tasks, evaluating the model using only the traditional loss function may not be sufficient to measure its generation quality, and the introduction of BLEU allows for a quantitative assessment of the model's text generation quality, especially in terms of syntactic and semantic accuracy. It measures the similarity between the `model-generated text (hypothesis)` and the `reference text (reference)` by comparing the two, with a score ranging from 0 to 1. A higher score indicates that the generated text is closer to the reference text."
      ],
      "metadata": {
        "id": "tDAqKmXdj5_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def bleuscore(reference, hypothesis):\n",
        "    return [nltk.translate.bleu_score.sentence_bleu([list(ref)], list(hyp)) for ref, hyp in zip(reference, hypothesis)]"
      ],
      "metadata": {
        "id": "524goGSiOcw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJSBtNGCX-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3732b489-40a9-4402-a86f-4fbbc2662d2c"
      },
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=256, lstm_hidden_size=512, lstm_layers=1, device='cuda')\n",
        "\n",
        "import time\n",
        "\n",
        "model.eval() # Tells PyTorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.2\n",
        "momentum = 0.99\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "batch_size = 64 if(not use_toy_dataset) else 128\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length() if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 50 if(not use_toy_dataset) else 20\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "time_0 = time.time()\n",
        "train_avg_loss = [] # will contain the average loss for each epoch for train\n",
        "dev_avg_loss = [] # will contain the average loss for each epoch for dev\n",
        "dev_avg_bleu_score = [] # will contain the average bleu score for each epoch for dev\n",
        "\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "\n",
        "  batch = batch_generator.get_batch(batch_size, subset=subset)\n",
        "  #print(ids_to_texts(batch[0])); print(ids_to_texts(batch[1]))\n",
        "  in_paragraphs = batch[0].to(model.device)\n",
        "  out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "  # For the loss, use torch.nn.functional.nll_loss. Computes an average over all tokens of the batch, but do not take into account distribution logits that corresonds to padding characters. Read the documentation and be careful about the shape of your tensors.\n",
        "\n",
        "  # compute the prediction\n",
        "  logits = model.trainingLogits(in_paragraphs, out_paragraphs) # (batch_size, max_length, size_vocabulary)\n",
        "  # compute the loss\n",
        "  loss = torch.nn.functional.nll_loss(logits.reshape(-1, logits.shape[-1]), out_paragraphs.reshape(-1), ignore_index=batch_generator.padding_idx)\n",
        "  # \"backward\" on the loss\n",
        "  loss.backward()\n",
        "  # store the loss in \"epoch_loss\"\n",
        "  epoch_loss.append(loss.item())\n",
        "\n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size):\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    # compute the average loss for each epoch\n",
        "    train_avg_loss.append(sum(epoch_loss) / len(epoch_loss))\n",
        "    print(f\"Average loss: {train_avg_loss[-1]}.\")\n",
        "\n",
        "    duration = time.time() - time_0\n",
        "    print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "\n",
        "    # Example of generation\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      batch = batch_generator.get_batch(1, subset=subset)\n",
        "      print(ids_to_texts(batch[0])) # Input paragraph\n",
        "      # Generated output paragraph\n",
        "      print(model.predictionStrings(batch[0].to(model.device), max_predicted_char=512))\n",
        "\n",
        "      # dev proccess\n",
        "      dev_loss = []\n",
        "      dev_bleu_score = []\n",
        "      for dev_batch in dev_batch_generator.all_batches(batch_size):\n",
        "        in_paragraphs = dev_batch[0].to(model.device)\n",
        "        out_paragraphs = dev_batch[1].to(model.device)\n",
        "        with torch.no_grad():\n",
        "          logits = model.trainingLogits(in_paragraphs, out_paragraphs) # (batch_size, max_length, size_vocabulary)\n",
        "          dev_loss.append(torch.nn.functional.nll_loss(logits.reshape(-1, logits.shape[-1]), out_paragraphs.reshape(-1), ignore_index=batch_generator.padding_idx).item())\n",
        "          texts = ids_to_texts(torch.argmax(logits, dim=-1))\n",
        "        #   texts, _ = model.predictionStrings(in_paragraphs, max_predicted_char=80) # for a normal size of dev dataset and max_predicted_char (which was 512), this operation will be too long !!!\n",
        "          # compute loss and bleu score\n",
        "          dev_bleu_score.append(np.mean(bleuscore(ids_to_texts(out_paragraphs), texts)))\n",
        "      # save average loss and bleu score\n",
        "      dev_avg_loss.append(sum(dev_loss) / len(dev_loss))\n",
        "      dev_avg_bleu_score.append(sum(dev_bleu_score) / len(dev_bleu_score))\n",
        "      print(f\"Dev loss: {dev_avg_loss[-1]}\")\n",
        "      print(f\"Dev bleu score: {dev_avg_bleu_score[-1]}\")\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.0275355138276754.\n",
            "19.303096055984497 s elapsed (i.e. 19.303096055984497 s/epoch)\n",
            "[\"D'ailleurs, elle devenait bien sentimentale. Il avait fallu échanger des miniatures, on s'était coupé des poignées de cheveux, et elle demandait à présent une bague, un véritable anneau de mariage, en signe d'alliance éternelle. Souvent elle lui parlait des cloches du soir ou des voix de la nature; puis elle l'entretenait de sa mère, à elle, et de sa mère, à lui. Rodolphe l'avait perdue depuis vingt ans. Emma, néanmoins, l'en consolait avec des mièvreries de langage, comme on eût fait à un marmot abandonné, et même lui disait quelquefois, en regardant la lune:\"]\n",
            "([\"7 cordait partait du pon de d'aut dan dur dui e due vant port le d'ant de mable lll'e cont dount dont dait cont di don dait d'uil de nat don dan d'ut s don de dain dout sas due s sa de comant tavie dant tait d'ont don dou d'ur suien por due d'ui sant d'art d'it sus aus pait con sancovale s saul parés du soi sons sout din d'ont se dui de le de due ront de cous de d'uit de rout dat le dur ant sait e dui sule du d'an due di d'ont ravoit torquent dus s d'ant sant que cont due d''ant landie prat du d'é di don das\"], tensor([-674.3566]))\n",
            "Dev loss: 2.5087254524230955\n",
            "Dev bleu score: 0.029605950086398013\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.2922123767234184.\n",
            "56.903136014938354 s elapsed (i.e. 28.451568007469177 s/epoch)\n",
            "[\"Quel débordement, le jeudi d'après, à l'hôtel, dans leur chambre, avec Léon! Elle rit, pleura, chanta, dansa, fit monter des sorbets, voulut fumer des cigarettes, lui parut extravagante, mais adorable, superbe.\"]\n",
            "([\"Emme s'andantres parcait la ca de la mant parnda trerins achait de la chermait, et déépour l'aune sui le vait parit paste ma maient lui à ma phar mont de l'en de pautes bil ambils Contre de ent de la parsient pertes, avait paut vaint jour comme lui l'autaimmant momme paux du parit qu'able le comme la mma s'aut don plur caute le la s'était aute cont la chavait aut dans et aventoient pars, en un et l'autre mait le poutiens tour l'aplu pluss parment pas sa paupre, et de véents pass, le le pour poutre le de se s\"], tensor([-678.2967]))\n",
            "Dev loss: 2.175468158721924\n",
            "Dev bleu score: 0.039289653719052776\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.0639117830678035.\n",
            "99.82211756706238 s elapsed (i.e. 33.27403918902079 s/epoch)\n",
            "[\"C'était de renouveler le billet signé par Bovary. Monsieur, du reste, agirait à sa guise; il ne devait point se tourmenter, maintenant surtout qu'il allait avoir une foule d'embarras.\"]\n",
            "([\"Ma comme plus de de grance parre pres, la barand son de se de ferche fait à se la la fendant de la ses matir par le pour la fait de l'alons ne verte de même tont mande faint au son des se marde moisson de comme de son le et de fertre pluss plus son pille au série plussi ces le de soin la ne comme aux de la vanc les par les des de les se fait de la vament de de des de comme au de fit de se des bante au marait plasses, et de parles des filles de marlait de varieur de au comme se au fent les paries de la jour l\"], tensor([-579.5651]))\n",
            "Dev loss: 2.0046093821525575\n",
            "Dev bleu score: 0.0573673982866774\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 1.9196388528153703.\n",
            "143.78473162651062 s elapsed (i.e. 35.946182906627655 s/epoch)\n",
            "[\"-- Allons, soyez franc! Nierez-vous qu'à Yonville...?\"]\n",
            "(['Elle se foir la comment, se la pous cousse les massait les dans se les pous; aver de roissait de la moille, le la comment des bassient de sans nouve plait de tour les des cont se pour de four les et que ou bon sesturs de la font la mule.'], tensor([-255.0697]))\n",
            "Dev loss: 1.8948322415351868\n",
            "Dev bleu score: 0.08991254842868694\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 1.8295944929122925.\n",
            "173.73351073265076 s elapsed (i.e. 34.746702146530154 s/epoch)\n",
            "['-- Si elle te demande de mes nouvelles, dit-il, tu répondras que je suis parti en voyage. Il faut remettre le panier à elle-même, en mains propres... Va, et prends garde!']\n",
            "([\"Et, elle rois de la faire, de la comme de casse des faire, et le restent falle se nous ne se ces contre de comme si de la cheur les pour la chantes des mournait un de voyant entre la comme un trouvait de coules était au marules plus les se la femme la feur la répont sa la comper tout pour au bruis comme la prose de la sagnement de comme dit-il avait ent la noute de gardissait lui et en la brassest-il au plaisper les peureur les dimaisaient à hamaisses de comme les s'embre le s'en pros sur lement sa frait la \"], tensor([-546.2165]))\n",
            "Dev loss: 1.8114607810974122\n",
            "Dev bleu score: 0.08091654081861364\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.7630474438538422.\n",
            "215.54063200950623 s elapsed (i.e. 35.92343866825104 s/epoch)\n",
            "[\"-- Il y en a d'autres, répondit-elle.\"]\n",
            "([\"-- Ah! ce n'y avais se présines de comme ressait plus en vois pas rivait contre par ce de la contre de la sembles enfois la cherchante de continie de la plus chappeille et un pisais de la pour le pas les pour couvers celle se sur couvent que les banchevait de la peluit du comme d'une banter les les elle la comme ressait de la par la fait des pertes dans le peur les remprice d'une sentait prendrant de sa comme de hautes de fine respant le se portemple ret chantement en de contourne de temple au de la piters d\"], tensor([-522.9260]))\n",
            "Dev loss: 1.748671531677246\n",
            "Dev bleu score: 0.06989920156382716\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.7071690688262116.\n",
            "258.9961848258972 s elapsed (i.e. 36.999454975128174 s/epoch)\n",
            "[\"-- C'est le fils de Boudet le charpentier; ses parents sont à leur aise et lui laissent faire ses fantaisies. Pourtant il apprendrait vite, s'il le voulait, car il est plein d'esprit. Et moi quelquefois, par plaisanterie, je l'appelle donc Riboudet (comme la côte que l'on prend pour aller à Maromme), et je dis même: mon Riboudet. Ah! ah! Mont-Riboudet! L'autre jour, j'ai rapporté ce mot-là à Monseigneur, qui en a ri... il a daigné en rire. -- Et M. Bovary, comment va-t-il?\"]\n",
            "([\"Il fait son appotait de sa trouvers de suissant de la voir des pâle se l'entre les tournait sans bruit qui se comme un grand une marissant de soir serait au bout le tout comme de les maissine une verre se contre les s'en basser les voyait que la contre du parasse, l'en moin des pour se prose de son jeur la conduit de des contre se présine de l'avait les de se condre mère lui proire de la rester de ses bout l'on le tendant de l'amportait une petine de son pouses, le peaux son prendre comme une par les vous le\"], tensor([-515.1968]))\n",
            "Dev loss: 1.7032537817955018\n",
            "Dev bleu score: 0.08482440780982405\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.6547872831946926.\n",
            "301.1648533344269 s elapsed (i.e. 37.64560666680336 s/epoch)\n",
            "[\"Déjà quelques-uns, qui se trouvaient arrivés, jouaient aux billes sur les dalles du cimetière. D'autres, à califourchon sur le mur, agitaient leurs jambes, en fauchant avec leurs sabots les grandes orties poussées entre la petite enceinte et les dernières tombes. C'était la seule place qui fût verte; tout le reste n'était que pierres, et couvert continuellement d'une poudre fine, malgré le balai de la sacristie.\"]\n",
            "(['«Au de la mainsier.'], tensor([-21.2937]))\n",
            "Dev loss: 1.659316062927246\n",
            "Dev bleu score: 0.07859476421819415\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.615826110582094.\n",
            "343.078298330307 s elapsed (i.e. 38.11981092558967 s/epoch)\n",
            "[\"«Et qu'aurais-je à faire, messieurs, de vous démontrer ici l'utilité de l'agriculture? Qui donc pourvoit à nos besoins? qui donc fournit à notre subsistance? N'est-ce pas l'agriculteur? L'agriculteur, messieurs, qui, ensemençant d'une main laborieuse les sillons féconds des campagnes, fait naître le blé, lequel broyé est mis en poudre au moyen d'ingénieux appareils, en sort sous le nom de farine, et, de là, transporté dans les cités, est bientôt rendu chez le boulanger, qui en confectionne un aliment pour le pauvre comme pour le riche. N'est-ce pas l'agriculteur encore qui engraisse, pour nos vêtements, ses abondants troupeaux dans les pâturages? Car comment nous vêtirions-nous, car comment nous nourririons-nous sans l'agriculteur? Et même, messieurs, est-il besoin d'aller si loin chercher des exemples? Qui n'a souvent réfléchi à toute l'importance que l'on retire de ce modeste animal, ornement de nos basses-cours, qui fournit à la fois un oreiller moelleux pour nos couches, sa chair succulente pour nos tables, et des oeufs? Mais je n'en finirais pas, s'il fallait énumérer les uns après les autres les différents produits que la terre bien cultivée, telle qu'une mère généreuse, prodigue à ses enfants. Ici, c'est la vigne; ailleurs, ce sont les pommiers à cidre; là, le colza; plus loin, les fromages; et le lin; messieurs, n'oublions pas le lin! qui a pris dans ces dernières années un accroissement considérable et sur lequel j'appellerai plus particulièrement votre attention.»\"]\n",
            "([\"-- Ahôte de la fermes de la fermes se bancondre les grande en ravert dans la feurent les boutes excure de les charmit à dans l'aprit de la près les bord son margeaux de sabond pider de ses bonnection de la chevec les faisse se jours le grandes contre comme était voules archant du modome l'avait venait des charmanches de manissait se chevant le regarets qu'il verre de la charmant la trois pour la charmait tout au fois dans les fait par les vous la fait l'était de la carri de ses son chabite du conville s'étai\"], tensor([-518.6840]))\n",
            "Dev loss: 1.6253676414489746\n",
            "Dev bleu score: 0.09488135307155093\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.5790190916312368.\n",
            "385.8124475479126 s elapsed (i.e. 38.58124475479126 s/epoch)\n",
            "[\"Quand le pharmacien n'entendit plus sur la place le bruit de ses souliers, il trouva fort inconvenante sa conduite de tout à l'heure. Ce refus d'accepter un rafraîchissement lui semblait une hypocrisie des plus odieuses; les prêtres godaillaient tous sans qu'on les vît, et cherchaient à ramener le temps de la dîme.\"]\n",
            "([\"Ce avec les contrait la partes de tendait la perdue, elle par des chaussait de basser un bournée de la chain de la perstait la pertit de son gous le trouvers de son maries de cournée de la mainsion de son et contre et de la feures par ses charma par son chez les sous l'exprit à la sa chamber, sur les pertites de la consider de terron les restendant lingter de la son lui appelions de la connusions de sa pristes de chautes aux partin de le chaspendait dépour les gendait sa trouvait des camps de la chambre des \"], tensor([-488.9480]))\n",
            "Dev loss: 1.5920254230499267\n",
            "Dev bleu score: 0.10080211566924394\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.5513670444488525.\n",
            "426.0672025680542 s elapsed (i.e. 38.73338205164129 s/epoch)\n",
            "[\"«Il me fait deuil de ne pas connaître encore ma bien-aimée petite-fille Berthe Bovary. J'ai planté pour elle, dans le jardin, sous ta chambre, un prunier de prunes d'avoine, et je ne veux pas qu'on y touche, si ce n'est pour lui faire plus tard des compotes, que je garderai dans l'armoire, à son intention, quand elle viendra.\"]\n",
            "([\"C'est peu le prochet que le vint la point en la tous les bourdin, et cela de leurs chapeau de ses pas choses d'eaux autres chambre, de terrie de les avait les posser les sallant de tout ce se prut tout se des pas pour se connuit à son contre son était son jeune passe de chambre de le portait pas vous la lui donnait les mangeait dans les charmacien d'un cour son affemmes à la jours. Elle en se rentait plus et comment à la son comment de leurs son connaire de la pendant les alleurs peu de vert des consin pour \"], tensor([-480.0370]))\n",
            "Dev loss: 1.5652547836303712\n",
            "Dev bleu score: 0.0950469098002181\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.5244245686029132.\n",
            "467.2511456012726 s elapsed (i.e. 38.93759546677271 s/epoch)\n",
            "['Ce furent trois jours pleins, exquis, splendides, une vraie lune de miel.']\n",
            "([\"au mari trouvaient des bassant de le point de l'amour elle ses bas de monde, le charbes de son chausse souvert marchant des chaser des servilles de sa connais de son voyait au mourna les chausse de tailles de la reste-même, je la cour des lui en semblait de la partire de sa maison autre de peu des grande des grande la paysant pour s'épaules de plaison d'avec des charchez ses doit des corchant d'autres avec un chasser la marchand de sa trois contre le chapes de la causer de chanches des calons de la promenait\"], tensor([-471.1227]))\n",
            "Dev loss: 1.5390806198120117\n",
            "Dev bleu score: 0.09703251420101171\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.4889296003290124.\n",
            "509.30008125305176 s elapsed (i.e. 39.17692932715783 s/epoch)\n",
            "[\"Et il se mit à fureter sur le lit, derrière les portes, sous les chaises; elle était tombée à terre, entre les sacs et la muraille. Mademoiselle Emma l'aperçut; elle se pencha sur les sacs de blé. Charles, par galanterie, se précipita et, comme il allongeait aussi son bras dans le même mouvement, il sentit sa poitrine effleurer le dos de la jeune fille, courbée sous lui. Elle se redressa toute rouge et le regarda par-dessus l'épaule, en lui tendant son nerf de boeuf.\"]\n",
            "([\"-- Eh ne par se plaissant de sa charge, avec le prendre de la maine et toutes cantendement, les arrivait son fois, tout des soirdent les habits, les moublier de son les par personnais de l'autre blanches du coup, restal. Elle avait passer les mobilets de maintendant d'angle continuait des plaisait en reprit la champille d'un coup de sa disait-il, et ces calard, et la boutes de la vertes, en cour se provante, en ses autres les chapeau à des étaient les mains, d'ailles sans si pressait des apperces, mais que l\"], tensor([-508.0809]))\n",
            "Dev loss: 1.5138610124588012\n",
            "Dev bleu score: 0.1053622459150256\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.46616961504962.\n",
            "550.7563669681549 s elapsed (i.e. 39.33974049772535 s/epoch)\n",
            "[\"-- Aussi, disait Rodolphe, je m'enfonce dans une tristesse...\"]\n",
            "([\"Mais les portes de la maris, et le pas de la taille de sa chausse de sa chaises de conte pas de levenait la plante de son mangeait de son consicité de son plus les arrivent à l'adressez de sa montre de son chaper de la voit de sa maisons de sa confemment de la comme de son bour, à la monter à la bourdin de la rappele sur la coute et les gons maladement d'autres de sa torte d'arresses de son combre toute la main, de si malade, et la cuisiner. Elle se s'arrêtait dans les tables du passé de la taille de ses pen\"], tensor([-432.0483]))\n",
            "Dev loss: 1.491028094291687\n",
            "Dev bleu score: 0.09478290332043204\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.4419131404475163.\n",
            "592.7939579486847 s elapsed (i.e. 39.51959719657898 s/epoch)\n",
            "[\"-- Embrassez-moi, dit l'apothicaire les larmes aux yeux. Voilà votre paletot, mon bon ami; prenez garde au froid! Soignez-vous! ménagez-vous!\"]\n",
            "([\"Elle se compagnant sans dépossions dans une main de bout de la cachette de la couler de l'amour avec le convendre sur les cours de l'assites lui déclations de reprit la car des envie. Ils y avait pas les considérait des campagnes.\"], tensor([-185.4556]))\n",
            "Dev loss: 1.467397654056549\n",
            "Dev bleu score: 0.10504555529856392\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.416246501175133.\n",
            "632.5551810264587 s elapsed (i.e. 39.53469881415367 s/epoch)\n",
            "[\"Mais la mère Bovary se récria bien fort sur ce nom de pécheresse. M. Homais, quant à lui, avait en prédilection tous ceux qui rappelaient un grand homme, un fait illustre ou une conception généreuse, et c'est dans ce système-là qu'il avait baptisé ses quatre enfants. Ainsi, Napoléon représentait la gloire et Franklin la liberté; Irma, peut-être, était une concession au romantisme; mais Athalie, un hommage au plus immortel chef-d'oeuvre de la scène française. Car ses convictions philosophiques n'empêchaient pas ses admirations artistiques, le penseur chez lui n'étouffait point l'homme sensible; il savait établir des différences, faire la part de l'imagination et celle du fanatisme. De cette tragédie, par exemple, il blâmait les idées, mais il admirait le style; il maudissait la conception, mais il applaudissait à tous les détails, et s'exaspérait contre les personnages, en s'enthousiasmant de leurs discours. Lorsqu'il lisait les grands morceaux, il était transporté; mais, quand il songeait que les calotins en tiraient avantage pour leur boutique, il était désolé, et dans cette confusion de sentiments où il s'embarrassait, il aurait voulu tout à la fois pouvoir couronner Racine de ses deux mains et discuter avec lui pendant un bon quart d'heure.\"]\n",
            "([\"La peures, et s'étaisse, de la construme, avec les partissait les grandes parpoles de conforts. Quand il faisait avec les considération ses pomportait des pharmaciens avec de son anterier des propostes à la cheveux fermes pour les comptes de la parlerier par le polition de la vertume du rendre de ses paroles des portes de café de la comme de la consulter cette en répondit comme un de la baison de curieure aux bras de ses doigts de ses petits avec des paraces de sa viste inconnaissait pour les cachettes de ca\"], tensor([-431.5959]))\n",
            "Dev loss: 1.4448276877403259\n",
            "Dev bleu score: 0.10966393644942571\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.3834657700438249.\n",
            "674.483517408371 s elapsed (i.e. 39.67550102402182 s/epoch)\n",
            "[\"Mais, soit qu'il n'eût pas remarqué cette manoeuvre ou qu'il n'eut osé s'y soumettre, la prière était finie que le nouveau tenait encore sa casquette sur ses deux genoux. C'était une de ces coiffures d'ordre composite, où l'on retrouve les éléments du bonnet à poil, du chapska, du chapeau rond, de la casquette de loutre et du bonnet de coton, une de ces pauvres choses, enfin, dont la laideur muette a des profondeurs d'expression comme le visage d'un imbécile. Ovoïde et renflée de baleines, elle commençait par trois boudins circulaires; puis s'alternaient, séparés par une bande rouge, des losanges de velours et de poils de lapin; venait ensuite une façon de sac qui se terminait par un polygone cartonné, couvert d'une broderie en soutache compliquée, et d'où pendait, au bout d'un long cordon trop mince, un petit croisillon de fils d'or, en manière de gland. Elle était neuve; la visière brillait.\"]\n",
            "([\"Emma, par le deux bouchet, le parlent d'autres ses ments à l'abandonner des visites de l'argent de la chasse de sans découpée des jours de son mariage d'avoir au basser sur les prendres, et le donnerait de la boulette, et le revenait des lettres. Il avait des longues de la tendre la fois, et le grand tant de la villa par des banches ou enfant de larme, et tout en se tenait trois cela, le peintendue les autres d'autres.\"], tensor([-379.7341]))\n",
            "Dev loss: 1.4246390461921692\n",
            "Dev bleu score: 0.11893447508673634\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.3571314586175454.\n",
            "713.5303809642792 s elapsed (i.e. 39.64057672023773 s/epoch)\n",
            "[\"-- Eh! non, car vous n'êtes pas une femme, vous.\"]\n",
            "([\"Et il se rengent de la campagne à l'autre des trois par les coules, la complit de la veille comme un coup d'elle avait pas. Mais il en faisait le rendonnait de ses deux grands de l'abandonnant en des paroles de sa figure de sa mains elle parler la présent en courut de la fine, elle l'est voulut à une présent de sa promets. Elle s'avait pas de pleine de son place, et la ferme, il se rappelait des conseils de la confinissement de son comptite à coup, qu'il ne devaine, prendre la figure de la vie par une fois d\"], tensor([-384.3931]))\n",
            "Dev loss: 1.4075955033302308\n",
            "Dev bleu score: 0.13005541205434007\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.3466623770563226.\n",
            "754.4290792942047 s elapsed (i.e. 39.70679364706341 s/epoch)\n",
            "[\"Il arrivait parfois des rafales de vent, brises de la mer qui, roulant d'un bond sur tout le plateau du pays de Caux, apportaient, jusqu'au loin dans les champs, une fraîcheur salée. Les joncs sifflaient à ras de terre, et les feuilles des hêtres bruissaient en un frisson rapide, tandis que les cimes, se balançant toujours, continuaient leur grand murmure. Emma serrait son châle contre ses épaules et se levait.\"]\n",
            "([\"Madame Bovary comme un temps de la casse, une semblait à ce gaugre comme la motine de charbes de la clache de la changent les parais un grand chalet de sa main s'échappart de monde chez les marches d'argent, et les cherchers.\"], tensor([-202.9719]))\n",
            "Dev loss: 1.3902709364891053\n",
            "Dev bleu score: 0.12358913653192286\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.3313143092232782.\n",
            "794.7669320106506 s elapsed (i.e. 39.73834660053253 s/epoch)\n",
            "['-- Savais-je que je vous accompagnerais?']\n",
            "([\"«Mais le médecin d'argent de rien de la carrette, et Emma première une veille. Il resta lui.\"], tensor([-85.0220]))\n",
            "Dev loss: 1.3767293572425843\n",
            "Dev bleu score: 0.13131302972431705\n",
            "-- END OF EPOCH 20.\n",
            "Average loss: 1.306517762106818.\n",
            "832.1859843730927 s elapsed (i.e. 39.62790401776632 s/epoch)\n",
            "[\"C'était un billet de sept cents francs, souscrit par elle, et que Lheureux, malgré toutes ses protestations, avait passé à l'ordre de Vinçart.\"]\n",
            "([\"Mais il prouvait sous la bouche, le certaine de plus tard longue de la rue, et de ce qui courrais qu'il voulut son aison d'organiser. Elle ne se rientait pas de son rappela de son bonheur d'en autre inclination d'abord de la tête de ses bassents de plus de son père de son père formait les deux basses de sa tête de son indignieur qui toutes les chambres de ces fils de la maison de sa rien.\"], tensor([-319.0645]))\n",
            "Dev loss: 1.3618468284606933\n",
            "Dev bleu score: 0.12076931574000001\n",
            "-- END OF EPOCH 21.\n",
            "Average loss: 1.2932541715471368.\n",
            "873.913102388382 s elapsed (i.e. 39.723322835835546 s/epoch)\n",
            "[\"Pour lui épargner de la dépense, sa mère lui envoyait chaque semaine, par le messager, un morceau de veau cuit au four, avec quoi il déjeunait le matin; quand il était rentré de l'hôpital, tout en battant la semelle contre le mur. Ensuite il fallait courir aux leçons, à l'amphithéâtre, à l'hospice, et revenir chez lui, à travers toutes les rues. Le soir, après le maigre dîner de son propriétaire, il remontait à sa chambre et se remettait au travail, dans ses habits mouillés qui fumaient sur son corps, devant le poêle rougi.\"]\n",
            "([\"Charles s'avançait son coup de la voiture d'un amour des lants, qui souvent des curies de beaucoup d'étresse la semble pour lui devant ses pommes de passions se récria-t-il de la porte et se passait des paroles de sa maison, il en tournait à la fournit de cet tous les pauvres des pieds, et des basseurs, et pour se confondit, et elle se continuait pas par les cours de la courante les pauvres places de chambre, et pour des dires, avait un grand constitude dans la voiture des deux bout les parmoires de la tendr\"], tensor([-374.6123]))\n",
            "Dev loss: 1.3522250413894654\n",
            "Dev bleu score: 0.1302757065308523\n",
            "-- END OF EPOCH 22.\n",
            "Average loss: 1.2648066185616158.\n",
            "912.9173080921173 s elapsed (i.e. 39.692056873570316 s/epoch)\n",
            "[\"Lorsqu'il s'aperçut donc que Charles avait les pommettes rouges près de sa fille, ce qui signifiait qu'un de ces jours on la lui demanderait en mariage, il rumina d'avance toute l'affaire. Il le trouvait bien un peu gringalet, et ce n'était pas là un gendre comme il l'eût souhaité; mais on le disait de bonne conduite, économe, fort instruit, et sans doute qu'il ne chicanerait pas trop sur la dot. Or, comme le père Rouault allait être forcé de vendre vingt-deux acres de son bien, qu'il devait beaucoup au maçon, beaucoup au bourrelier, que l'arbre du pressoir était à remettre:\"]\n",
            "([\"-- Ah! bien dit-elle en suis que l'habitude de ses couris, et il se lentait devaient en la main se laissait à la foule de ses boutes sous les deux mains sur la feuet sur la cournier de la cande que ce qui trouvait pas, s'absentait à son ancie de la bonne faisait que l'entrant de leurs hauteurs tous les foules de rende et l'existe de la salle en des boutons de son chapeau de plus que l'on parayaissait le calme de la plus de la main à la langue, les charrettes avaient des rancons de fermes des canses pour la f\"], tensor([-415.1093]))\n",
            "Dev loss: 1.3372995018959046\n",
            "Dev bleu score: 0.1340533133556416\n",
            "-- END OF EPOCH 23.\n",
            "Average loss: 1.2492159793251438.\n",
            "953.1091027259827 s elapsed (i.e. 39.712879280249275 s/epoch)\n",
            "[\"-- Non, mais je l'aime beaucoup, répondit-il.\"]\n",
            "([\"Elle se rentrait de tout le couleur qui serait le long des tables de la carte de porte lui et de la route de ses pitiétés de la première quelque chose du licot de la cheminée de couleurs du cieux, près de lui-même, de soit en regardant le soir, et le soir de la porte de couter son entrente la poitrine. Les mains de bassent de son ort de tout au fort de prairie de la tête de cris de la famille de la chaise, au bord de la main de la cause de cuir son petit rien d'un air plus parti d'une chaise maintenant, de l\"], tensor([-375.4762]))\n",
            "Dev loss: 1.3240800619125366\n",
            "Dev bleu score: 0.144628069303987\n",
            "-- END OF EPOCH 24.\n",
            "Average loss: 1.2378670234937925.\n",
            "993.5703840255737 s elapsed (i.e. 39.74281536102295 s/epoch)\n",
            "['-- Ah! se dit-elle, il porte un couteau dans sa poche, comme un paysan!']\n",
            "(['Le francs baiser.'], tensor([-21.2917]))\n",
            "Dev loss: 1.3141533851623535\n",
            "Dev bleu score: 0.15019453761960605\n",
            "-- END OF EPOCH 25.\n",
            "Average loss: 1.2205155701250643.\n",
            "1031.742966890335 s elapsed (i.e. 39.682421803474426 s/epoch)\n",
            "['Elle le trouva dans sa boutique, en train de ficeler un paquet.']\n",
            "([\"Le langueur d'un soleil, toutes les familles des conforces de son appartement d'avantage de la lumière au bout de découvrir des assoupissantes, comme un peu d'une autre foule de son principer de temps à autre de la faire venait au belle qu'elle aperçut en des femmes, et elle lui remuait dans la faille de sa champe, et de terre, en effaçonçait de la cheminée de la conduire au bout de la cuisine, il mangeait, et s'attendait au bout de la facture des rélevés de la morage de faire connaisse de ses places d'exist\"], tensor([-368.7542]))\n",
            "Dev loss: 1.305560839176178\n",
            "Dev bleu score: 0.1494101926469129\n",
            "-- END OF EPOCH 26.\n",
            "Average loss: 1.209414237423947.\n",
            "1070.8905899524689 s elapsed (i.e. 39.66261444268403 s/epoch)\n",
            "[\"À l'époque de la Saint-Michel, Charles était venu passer trois jours aux Bertaux. La dernière journée s'était écoulée comme les précédentes, à reculer de quart d'heure en quart d'heure. Le père Rouault lui fit la conduite; ils marchaient dans un chemin creux, ils s'allaient quitter; c'était le moment. Charles se donna jusqu'au coin de la haie, et enfin, quand on l'eut dépassée:\"]\n",
            "(['-- Ah! son mari.'], tensor([-12.0415]))\n",
            "Dev loss: 1.3011865019798279\n",
            "Dev bleu score: 0.17914567788196073\n",
            "-- END OF EPOCH 27.\n",
            "Average loss: 1.2025375108461123.\n",
            "1103.4956467151642 s elapsed (i.e. 39.41055881125586 s/epoch)\n",
            "['Il claqua de la langue. Les deux bêtes couraient.']\n",
            "([\"Elle lui remuait ses arranger des deux descendres de la glace, lui paraissait de la poitrine, et des femmes, elles étaient tous les sens de toutes les femmes à la grosse mots de son homme de sa femme, elle le parut des fleurs de sa main sur la vieille porte de son charret, et comme le soleil de sa femme de se regarder des balets d'argent de colonditions de se reversaient de la cuisine se montait envie, les deux bouillons sous les places de la chaire. Il avait l'apothicaire de la solenne et se vous avec des m\"], tensor([-338.9442]))\n",
            "Dev loss: 1.2931896090507506\n",
            "Dev bleu score: 0.1376558151384823\n",
            "-- END OF EPOCH 28.\n",
            "Average loss: 1.1836290171271877.\n",
            "1143.848790884018 s elapsed (i.e. 39.44306175462131 s/epoch)\n",
            "['-- Où Monsieur va-t-il? demanda le cocher.']\n",
            "([\"-- Ce m'en ai pas, en trouve plus impossible que l'on en a des parents se retournaient à la porte de la façon de se rencher des autres aux délicatesses de la petite fine de ses rideaux un peu de ses pauvres temps à autre, n'était pas les répons de cortillon de respecte de la fauteuil en lui parut, en petit bien pensée à petites positions qui les considéraient à son père de la poitrine. Cela dit le coeur de l'autre, il se répétait à son petit soufflait de son tous les plus loints de sa poitrine en bouteille. \"], tensor([-347.2669]))\n",
            "Dev loss: 1.2886168241500855\n",
            "Dev bleu score: 0.1715321687386861\n",
            "-- END OF EPOCH 29.\n",
            "Average loss: 1.1814441229846027.\n",
            "1181.3242964744568 s elapsed (i.e. 39.37747654914856 s/epoch)\n",
            "[\"-- Il me semble que c'est tout. Ah! encore ceci, de peur qu'elle ne vienne à me relancer:\"]\n",
            "([\"-- Et qu'il ne savait pas le soir, par la fenêtre.\"], tensor([-25.0873]))\n",
            "Dev loss: 1.2855706810951233\n",
            "Dev bleu score: 0.19265192690480676\n",
            "-- END OF EPOCH 30.\n",
            "Average loss: 1.1669151563393443.\n",
            "1214.3792777061462 s elapsed (i.e. 39.17352508729504 s/epoch)\n",
            "[\"-- Madame Bovary! fit Homais. Je m'empresse d'aller lui offrir mes hommages. Peut-être qu'elle sera bien aise d'avoir une place dans l'enceinte, sous le péristyle.\"]\n",
            "([\"-- Ah! non, cela n'est pas trop repas, sans doute, c'est qu'il n'avait pas continuant les cheveux son profondeurs encores au milieu de la compagnie des choses personnales se représissaient du feu de la quaité de hauteurs avances de ses chevaux de bouchon ou front au moins de candeur de la communication de confondues par le bonhomme deux chambres à cette immense raisonnal de sa tête et qui fait des parles, comme leurs frois du pharmacien, il descendit en s'entendre une petite femme du premier au coin de l'exa\"], tensor([-368.2890]))\n",
            "Dev loss: 1.2804750680923462\n",
            "Dev bleu score: 0.15903841204596597\n",
            "-- END OF EPOCH 31.\n",
            "Average loss: 1.1613917028581775.\n",
            "1253.3621277809143 s elapsed (i.e. 39.16756649315357 s/epoch)\n",
            "['-- Emma...']\n",
            "([\"Alors, de bras sur la bouche du milieu droit de l'heure d'un de reste en regardant les conseils du soleil de la main de la cuissière de la large par un bouquet la campagne de la grande table de son coeur de l'estradame de l'eau de la grande robe de pays. Elle envoyait en son part les hers, et les larmes à la rue. Elle se trouvait à touces de son or, les fournissements de la chambre. Il avait son cou magnifique de cette passion qu'il fassait pas les couvents, les volailles en plus son obscurité de sentir tout\"], tensor([-402.9726]))\n",
            "Dev loss: 1.2729814410209657\n",
            "Dev bleu score: 0.1621913702815046\n",
            "-- END OF EPOCH 32.\n",
            "Average loss: 1.1441655062340401.\n",
            "1291.6176056861877 s elapsed (i.e. 39.139927445035994 s/epoch)\n",
            "['-- Si elle te demande de mes nouvelles, dit-il, tu répondras que je suis parti en voyage. Il faut remettre le panier à elle-même, en mains propres... Va, et prends garde!']\n",
            "([\"-- Si vous en aurait incannerait pour moi! dit Bournisien de la culture et de la muraille et des réfléchirs et le petit trandique ses mains, et les enfants de la ferme, et les mains, et les pères de la porte, et l'on voyait son lit dans la conte, ses personnes de son correvoir. Sa peau d'une conversation de sa main, elle se trouvait.\"], tensor([-244.3506]))\n",
            "Dev loss: 1.2686091780662536\n",
            "Dev bleu score: 0.1723653138831311\n",
            "-- END OF EPOCH 33.\n",
            "Average loss: 1.1336293440116079.\n",
            "1327.0518219470978 s elapsed (i.e. 39.03093593962052 s/epoch)\n",
            "['Souvent il faisait des exclamations:']\n",
            "(['-- Ah! non, mais plus!'], tensor([-15.0550]))\n",
            "Dev loss: 1.2695015907287597\n",
            "Dev bleu score: 0.1613895113893838\n",
            "-- END OF EPOCH 34.\n",
            "Average loss: 1.1312106525575794.\n",
            "1365.0884354114532 s elapsed (i.e. 39.00252672604152 s/epoch)\n",
            "[\"-- J'aurais besoin d'argent.\"]\n",
            "([\"-- Je mais qui t'a demande de la maison de la maison de la maison de cheval de ronflement de la cambrade de tout cela traversait son amour, par aimer. Elle se trouva dans les cours, de la maison dans un salon, lorsqu'il avait souffrir cette profitation de sa femme de la route, se rapprochent de la lampe, sous la porte du port, de sa cours, et son coup de sa personne dans une manière de plus étalé de cette professe, de tous les jours de loin, son doigt les principas de petits pouvaines femmes, et le feu, sa r\"], tensor([-328.7180]))\n",
            "Dev loss: 1.265057110786438\n",
            "Dev bleu score: 0.2122983686689081\n",
            "-- END OF EPOCH 35.\n",
            "Average loss: 1.1150106505343789.\n",
            "1399.7072796821594 s elapsed (i.e. 38.880757768948875 s/epoch)\n",
            "[\"Et Lheureux tira de son coffre-fort le reçu de dix-huit cents francs, qu'elle lui avait donné lors de l'escompte Vinçart.\"]\n",
            "([\"Quelque chose de certains molles vertes, je me partirle de cette montre considérable! se disait-elle en profitant au commencement de l'eau point de se couvrit pour le monde de l'autre particulier de la chaise.\"], tensor([-136.5381]))\n",
            "Dev loss: 1.2605045914649964\n",
            "Dev bleu score: 0.20503015245873524\n",
            "-- END OF EPOCH 36.\n",
            "Average loss: 1.106539271973275.\n",
            "1434.9224026203156 s elapsed (i.e. 38.781686557305825 s/epoch)\n",
            "[\"Au fond de son âme, cependant, elle attendait un événement. Comme les matelots en détresse, elle promenait sur la solitude de sa vie des yeux désespérés, cherchant au loin quelque voile blanche dans les brumes de l'horizon. Elle ne savait pas quel serait ce hasard, le vent qui le pousserait jusqu'à elle, vers quel rivage il la mènerait, s'il était chaloupe ou vaisseau à trois ponts, chargé d'angoisses ou plein de félicités jusqu'aux sabords. Mais, chaque matin, à son réveil, elle l'espérait pour la journée, et elle écoutait tous les bruits, se levait en sursaut, s'étonnait qu'il ne vînt pas; puis, au coucher du soleil, toujours plus triste, désirait être au lendemain.\"]\n",
            "([\"L'apothicaire approchait des basses bleus, se répondit à l'autre, il y avait un peu les passions de leur confondues d'argent plus la bouche de toutes les jours d'une autre fille, et qui avait appris de la sentir chez l'exprimer les charrettes, et l'on retrouverait dans la foule de sa femme se penchaient dans les choses de sa femme et s'arrêtant à la conversation qui était passé comme des bonnets de grands passeronts sur les planches ou de bonne heure, du menton baillard, qui savait les conseils de la guette \"], tensor([-310.2756]))\n",
            "Dev loss: 1.2627168893814087\n",
            "Dev bleu score: 0.2092391554831273\n",
            "-- END OF EPOCH 37.\n",
            "Average loss: 1.1033699732077749.\n",
            "1467.8269107341766 s elapsed (i.e. 38.62702396668886 s/epoch)\n",
            "[\"La foule arrivait dans la grande rue par les deux bouts du village. Il s'en dégorgeait des ruelles, des allées, des maisons, et l'on entendait de temps à autre retomber le marteau des portes, derrière les bourgeoises en gants de fil, qui sortaient pour aller voir la fête. Ce que l'on admirait surtout, c'étaient deux longs ifs couverts de lampions qui flanquaient une estrade où s'allaient tenir les autorités; et il y avait de plus, contre les quatre colonnes de la mairie, quatre manières de gaules, portant chacune un petit étendard de toile verdâtre, enrichi d'inscriptions en lettres d'or. On lisait sur l'un: «Au Commerce»; sur l'autre: «À l'Agriculture»; sur le troisième: «À l'Industrie»; et sur le quatrième: «Aux Beaux-Arts».\"]\n",
            "([\"Elle soulia sans doute en avantage, des bas de ses trois fournieux bandonner. Les mains de la cheminée la main sur la fortune qui se trouva la ferme la polite, s'était raconter la terre du bas de son enfant.\"], tensor([-150.6775]))\n",
            "Dev loss: 1.258729374408722\n",
            "Dev bleu score: 0.22756499453417836\n",
            "-- END OF EPOCH 38.\n",
            "Average loss: 1.0966621057407275.\n",
            "1499.2818458080292 s elapsed (i.e. 38.44312425148793 s/epoch)\n",
            "[\"Il y avait même des jours où, à peine rentrée, elle montait dans sa chambre; et Justin, qui se trouvait là, circulait à pas muets, plus ingénieux à la servir qu'une excellente camériste. Il plaçait les allumettes, le bougeoir, un livre, disposait sa camisole, ouvrait les draps.\"]\n",
            "([\"-- Ah! moi, j'ai pas de se récrire. Les lettres en porter la contre son cours de sa camparet de sa personne, et pour la cassouble.\"], tensor([-100.7598]))\n",
            "Dev loss: 1.2616146326065063\n",
            "Dev bleu score: 0.14378301662813545\n",
            "-- END OF EPOCH 39.\n",
            "Average loss: 1.0857641245867755.\n",
            "1538.7290589809418 s elapsed (i.e. 38.46822647452355 s/epoch)\n",
            "['-- Quoi donc?']\n",
            "(['-- Eh bien... reprit-il en se disait-elle.'], tensor([-16.3644]))\n",
            "Dev loss: 1.2573260545730591\n",
            "Dev bleu score: 0.19972688372132988\n",
            "-- END OF EPOCH 40.\n",
            "Average loss: 1.0797447844555503.\n",
            "1574.203234910965 s elapsed (i.e. 38.39520085148695 s/epoch)\n",
            "[\"Il n'osait lui faire des questions; mais, la discernant si expérimentée, elle avait dû passer, se disait-il, par toutes les épreuves de la souffrance et du plaisir. Ce qui le charmait autrefois l'effrayait un peu maintenant. D'ailleurs, il se révoltait contre l'absorption, chaque jour plus grande, de sa personnalité. Il en voulait à Emma de cette victoire permanente. Il s'efforçait même à ne pas la chérir; puis, au craquement de ses bottines, il se sentait lâche, comme les ivrognes à la vue des liqueurs fortes.\"]\n",
            "([\"Par le lendemain des vêtements par les portes de sa bouche au bord du coeur, par les deux bassant et les mains du pays, au coin du marche qui n'en rentrait pas ma pas du point des cornes douces de la vie, et qui portaient des conseils d'autres choses une chaise de s'approchant de la corde sous les campagnes de la porte, les conseils dans les couvertures, dans le salet, au moindre du marchande d'une regarde et des mains à poussière, au parapieur des cheveux de la suite; il avait une poignée de vieux doigt que\"], tensor([-358.6110]))\n",
            "Dev loss: 1.2594080924987794\n",
            "Dev bleu score: 0.2027039630449649\n",
            "-- END OF EPOCH 41.\n",
            "Average loss: 1.0637597844407365.\n",
            "1609.2263946533203 s elapsed (i.e. 38.314914158412385 s/epoch)\n",
            "['-- Ça ne nuit jamais, répliqua-t-il.']\n",
            "([\"-- Mais ce sont le voir son échapporté, je le me sonnerait pas de la première communicale d'argent.\"], tensor([-65.1908]))\n",
            "Dev loss: 1.2548070073127746\n",
            "Dev bleu score: 0.21442393931855858\n",
            "-- END OF EPOCH 42.\n",
            "Average loss: 1.0626717147074247.\n",
            "1640.7707011699677 s elapsed (i.e. 38.15745816674343 s/epoch)\n",
            "[\"Elle remua la tête en signe d'assentiment; puis, un quart d'heure après:\"]\n",
            "([\"-- Ah! c'est fini! par son amour.\"], tensor([-20.2099]))\n",
            "Dev loss: 1.2580268383026123\n",
            "Dev bleu score: 0.19810988127246015\n",
            "-- END OF EPOCH 43.\n",
            "Average loss: 1.0510118297628455.\n",
            "1674.4353029727936 s elapsed (i.e. 38.055347794836216 s/epoch)\n",
            "[\"Et madame Bovary, non plus que Rodolphe, ne lui répondait guère, tandis qu'au moindre mouvement qu'ils faisaient, il se rapprochait en disant: «Plaît-il?» et portait la main à son chapeau.\"]\n",
            "([\"-- Et peu à autrefois, de mon pauvre ans, au coin du monde, il restait à la porte de l'autre en lanterne la perme de sa santé, qui avait réussé, et elle se désirait:\"], tensor([-107.6575]))\n",
            "Dev loss: 1.2559762954711915\n",
            "Dev bleu score: 0.23378292591445496\n",
            "-- END OF EPOCH 44.\n",
            "Average loss: 1.0482903767276455.\n",
            "1706.6537194252014 s elapsed (i.e. 37.92563820944892 s/epoch)\n",
            "[\"Madame Bovary mère, la veille au soir, en traversant le corridor, l'avait surprise dans la compagnie d'un homme, un homme à collier brun, d'environ quarante ans, et qui, au bruit de ses pas, s'était vite échappé de la cuisine. Alors Emma se prit à rire; mais la bonne dame s'emporta, déclarant qu'à moins de se moquer des moeurs, on devait surveiller celles des domestiques.\"]\n",
            "([\"C'était le clerc les charces qu'elle avait la taille, après la bouche, comme la catation de ces ménages, le front à la literne d'un seul mot à coup d'oeil qu'une fois maigre, en petit plein de ferme, nous autres les garderons noirs s'arrêtaient des campagnes sur le mur des pieds sur son autre, et quelque chose de subtime ou plus long que la contemplation de la chose, en contemplant les saints sont trop lointains de l'ombre des mains, se permit par le corde des gazouilles, comme des décorses, et les mains de \"], tensor([-377.7640]))\n",
            "Dev loss: 1.2591442823410035\n",
            "Dev bleu score: 0.19616454929822252\n",
            "-- END OF EPOCH 45.\n",
            "Average loss: 1.0441029918821234.\n",
            "1744.1550912857056 s elapsed (i.e. 37.91641502795012 s/epoch)\n",
            "[\"Quand le café fut servi, Félicité s'en alla préparer la chambre dans la nouvelle maison, et les convives bientôt levèrent le siège. Madame Lefrançois dormait auprès des cendres, tandis que le garçon d'écurie, une lanterne à la main, attendait M. et madame Bovary pour les conduire chez eux. Sa chevelure rouge était entremêlée de brins de paille, et il boitait de la jambe gauche. Lorsqu'il eut pris de son autre main le parapluie de M. le curé, l'on se mit en marche.\"]\n",
            "([\"Madame Bovary, quand le front, sous la confidence de la médecine se retenait au commencement. Enfin, sous le sapin, des bourgeoises qui s'en allaient au commencement. Il ne se prompait pas de bance sur la communion.\"], tensor([-117.3562]))\n",
            "Dev loss: 1.2631839513778687\n",
            "Dev bleu score: 0.19353689242722436\n",
            "-- END OF EPOCH 46.\n",
            "Average loss: 1.033720370885488.\n",
            "1779.176964044571 s elapsed (i.e. 37.85482902222491 s/epoch)\n",
            "['-- Avec un sac de nuit.']\n",
            "([\"-- Eh! c'est pour la grande récolte à protestation, et, pressant le professeur, sur la place du pied, sur la tente.\"], tensor([-77.8891]))\n",
            "Dev loss: 1.2612636923789977\n",
            "Dev bleu score: 0.19667675927038414\n",
            "-- END OF EPOCH 47.\n",
            "Average loss: 1.020864808245709.\n",
            "1813.5931491851807 s elapsed (i.e. 37.7831906080246 s/epoch)\n",
            "[\"-- Comme j'ai été sage! se disait-elle en songeant aux écharpes.\"]\n",
            "([\"Ce qui s'appelle un peu par cette position au collet de la langue, les conseils du chapeau qui tombait. Elle l'appelait les yeux la campagne de chez les femmes, et les paysans ne venaient pas de pousser la porte des chaussures qui lui couraient dans le comptoir, comme le préfet se laisser par les deux jours de l'autre. Elle est malade, se dépassant des considérations de son mari, qui la regardait de ses malades, et des convictions au coudant des couvertures d'un chapeau sur la porte des pompes de cours, avec\"], tensor([-273.3803]))\n",
            "Dev loss: 1.2659672975540162\n",
            "Dev bleu score: 0.2126922707592694\n",
            "-- END OF EPOCH 48.\n",
            "Average loss: 1.0152051561587565.\n",
            "1847.998776435852 s elapsed (i.e. 37.71426074358882 s/epoch)\n",
            "[\"-- Mes amis? lesquels donc? en ai-je? Qui s'inquiète de moi?\"]\n",
            "([\"-- C'est le marquis de demi-voix.\"], tensor([-18.0627]))\n",
            "Dev loss: 1.2741496205329894\n",
            "Dev bleu score: 0.19713202077542474\n",
            "-- END OF EPOCH 49.\n",
            "Average loss: 1.0112021157616062.\n",
            "1882.9175317287445 s elapsed (i.e. 37.65835063457489 s/epoch)\n",
            "[\"Charles se récria encore une fois qu'il ne pouvait s'absenter plus longtemps; mais rien n'empêchait Emma...\"]\n",
            "([\"-- Sais-tu de venir, reprit le bonheur qui continua le col de la cheminée de la maison du pied dans la voix de la peau ternime, souvent il se rendit pas. Il l'avait toujours sur la terre des convoitises, elle avait apporté le commerce; elle alla pourtant sur le campagne des doigts la porte de la commode pour le voyage, il entrent en dernier comme le carreau se répandit avec la convenance, le conseil de son chapeau dans la cuisine de la botte entre. Emma voulut se renvoyer en trouver la veille, et elle répéta\"], tensor([-283.0152]))\n",
            "Dev loss: 1.2707170367240905\n",
            "Dev bleu score: 0.18929117362202477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxs4UZRAHxx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fcdfc5-f6c6-44a5-ef8a-c0e2325fbd84"
      },
      "source": [
        "with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "  if(use_toy_dataset):\n",
        "    prompt = [\"AAAA. Now, please write 3 i.\" + EOP] #[paragraphs[0]]\n",
        "    print(prompt[0])\n",
        "    print(f\"(Is this prompt in the training set? {prompt[0] in paragraphs})\\n\")\n",
        "\n",
        "    for _ in range(10):\n",
        "      batch = batch_generator.turn_into_batch(prompt)\n",
        "      gen_texts, logprobs = model.predictionStrings(batch.to(model.device), max_predicted_char=128)\n",
        "\n",
        "      print(f\"{gen_texts[0]} (log-probability: {logprobs.item()})\")\n",
        "      prompt = [gen_texts[0] + EOP]\n",
        "  else:\n",
        "    prompt = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"] * 10\n",
        "    batch = batch_generator.turn_into_batch(prompt)\n",
        "    gen_texts, logprobs = model.predictionStrings(batch.to(model.device), max_predicted_char=1024)\n",
        "\n",
        "    print(prompt[0])\n",
        "    print()\n",
        "    for i, (gen_text, logprob) in enumerate(zip(gen_texts, logprobs)):\n",
        "      print(f\"{i}: \", end=\"\")\n",
        "      print(gen_text)\n",
        "      print(f\"log-probability: {logprob.item()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\n",
            "\n",
            "0: -- Sortez! dit Emma.\n",
            "log-probability: -614.4078979492188\n",
            "1: Emma retrouvait la berge; elle le revint en passant sur la porte de la poitrine, et la classe de prenait, elle s'envolait aux places d'argent. Elle resta signé à la porte à cette basse de la cheminée qui la rendit en pleine avec son époux la première fois ses premiers salements. Elle se rencontra par un air de soleil de la mariage était un morteau de vent, et, du marquis, de la cuisine, avec les détespentiers, des cordons de marchand de pluie comme des couleurs sur les balances de peur qui tombe de comme des balances de papier.\n",
            "log-probability: -639.3671264648438\n",
            "2: -- Quelle est beaucoup de ce qui est bien pour elle tout ce qui entre à ce traîter pour la nuit, elle avait accusé de la campagne de sa commodité sa malade où elle se rappela des pas lourds, le père Rolet, que lui demanda d'un air d'amour continuellement sa provision de précautions près d'elle-même, et qui se trouvait la tête d'une bride se considérer la convenance retirait la marquise de Bovary. Les arts, par un de ces cours de paysanne. Peut-être avant d'ailleurs, ce serait passer des commises de coeur que le colone des communes en offrir, la taille basse en couleur sur la table, avec une rideau de velours qui souriait dans un conseil malade. Elle avait des découvrages de pluie qu'un chapeau de plus suivaient le balance entre les carreaux des fermes, et il revint la tremblante de l'appeler, et il se rappela le pharmacien dont la convenue. Elle ne riait pas de la campagne et comme si tu es tourner ma demande, dont la voix de ce précipiter, avec un ridel noir, et cela doit la première soupe de la conversation \n",
            "log-probability: -618.89111328125\n",
            "3: -- Cela ne pouvais rien, ne pas assez donc? reprit-elle, en voyant ses dents donces! Il se rencontre que ce n'était pas la peau du commerce.\n",
            "log-probability: -610.7035522460938\n",
            "4: -- Mais cette amoureux de temps à artiste, ce qui ne pousse la mari de le vendre qu'elle n'avait pas se réconter les mains dans la chambre, et le consentement de la pelée de la ville, la porte de son corsage, les meubles passant sa poste de billard, elle alla contemplant l'affaire de la conscience de la vie par un tour passant pour vous le commerce, elle s'aperçut remontier des printes. Elle est la porte de la cuisine en sorte de papier coquet, et qui m'a prochaine, ce soir, aux répétiens de la cassable. Il la suissit le commerce. La nuit s'évanouir de la procuration et sa main sur le bonheur de la tonnelle. Le bonheur qui passait en lui faire des passions silencieuses, les femmes en bonnets de coton, les marches de la chambre, de cette idée qu'elle avait causé tout en se tenir assassue. La bourras plus la semaine de se rendre plus tard de ce qui l'aime dont elle avait fait convenir de la cheminée était le collet de la mare de ces princes de rendez-vous par le regard tous les premiers de sa chambre, et elle s'\n",
            "log-probability: -613.6256713867188\n",
            "5: Rodolphe avait encore une fois qu'il en avait auprès des manches de paysanne. Mais une religiant comme les coux de la prairie, où il était en venade. Elle entre en trouver la tête, et, aussitôt, il ne parlait pas de coucher, vers le soir, il se rappela ses pantoufles en voix de poltes pour les convenances passer des considérations qui se rappelaient une chambre de ferme, et elle restait encore la stréphendopodie en répétatit un archeter de sa porte. Il regardait la consolation de l'argent.\n",
            "log-probability: -617.9368896484375\n",
            "6: On arriva pour qui ne pas la cassé de sa chambre, au milieu d'une colonne de marche, la convenance de plus de passer les moeurs et qui montait des chiens d'argent.\n",
            "log-probability: -646.8745727539062\n",
            "7: Il rentrait dans sa chambre, elle se pensait de sa femme d'avance se rivant, elle avait cassé sur les autres entrententes de la mairie, on devait avoir la perte de son ombrelle, et il se remettre de la mare ombre des grâces qui la personne aux pieds de la commode comme un simple pour lui des yeux plus lointes, dans la chambre, elle avait revoir la première partique à la foin de la porte de la délicité de pommade, et qui avait été brun, ses dents contre le vent, où le temps de la cour et des regards pas de marcher. Les bourgeoises d'argent, elle avait désiré sa pose sur la terre des balances qui la mueillent donc la porte de sa connaissance et de la cause de la main dans les arbres, et le monde en lui disait quelque chose d'eau de cette inconnu. Emma reprit:\n",
            "log-probability: -677.1065063476562\n",
            "8: -- Emma! reprit la mare Homais en souria donc qu'il était passée; par les rendez-vous, comme son chapeau de noir, entre les doigts se tourner avec son chapeau d'une marchand doigt son costume la changeait de rendez-vous. Emma ne l'avait pas sa masse de billard, et le regard allait, en se rencontrant comme une façe de bonheur qui partit au milieu de la cuisine de ce tinte et qui avait apporté subtil avec une sorte de collège blanche, et qui avait des articuls par le pays de la nature de la boutique. Je revint à pas, sa mariage, de la commune et de la vie de l'angle de la mariage, en se rencontrant pâliment son bonnet de son bruisse, en rofflement encore aux plantes de la cheminée pour les portes de cuivre et de sa cuisine. Pelait les marches de la place de la cheminée envie de ce rame et rentre la fenêtre, et le bonhet avait attenu; le jeune homme entrait. Elle s'en allait alors son coeur sur ses doigts. Elle se rencontre en l'écarte de la commande, et qui le regardait, et le trouva la pendule de l'argent.\n",
            "log-probability: -618.9218139648438\n",
            "9: On entendit la bonne forte de la vie passée; l'angle du soleil fumant des confondures, la consolait, et sa femme du médecin suivait la tête comme des confondues d'amour, la première fois de la commande de la commune sur la cheminée de la porte et de son coeur de poussière. Elle entra dans les servillants, de la maison, la promenade donc en passant entre les arches encore soleil de la commune, et elle rentrait autrefois, en repoussa la main quand il fut surprise de sa maladie, elle avait des parties bouges de collène, et, passant sa main pour le ventement de la mairie, elle se renversait les mains de l'année, et qui se rappela le maître.\n",
            "log-probability: -646.9869995117188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define a function `visualise()` to visualize the training process and the prediction results. It is used to plot the Loss and BLEU Score of the model with the increase of epoch during the training process, such a global view is convenient for us to find the problems and adjust the model and hyperparameters."
      ],
      "metadata": {
        "id": "KXOPhO8xsZ28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualise(train_loss, dev_loss, bleu_score):\n",
        "    # plot loss over epochs\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(list(range(len(train_loss))) , train_loss, label='Train Loss', marker='o', linestyle='-', color='orange')\n",
        "    plt.plot(list(range(len(dev_loss))) , dev_loss, label='Dev Loss', marker='o', linestyle='-', color='green')\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # plot bleu score over epochs\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(list(range(len(bleu_score))), bleu_score, label='BLEU Score', marker='o', linestyle='-', color='blue')\n",
        "    plt.title(\"BLEU Score over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"BLEU Score\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PmOr1D6XOXW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualise(train_avg_loss, dev_avg_loss, dev_avg_bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "0ZMxJAn1SvpB",
        "outputId": "cda0e19d-e56b-4db8-8eb7-ee9703aefdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph, we can do the following analysis: when the number of epochs is initially 20, the model can effectively learn the features of the training data, the loss values of both the training and dev sets are decreasing steadily, and the BLEU scores of the dev set are gradually increasing, which indicates that the model has not yet overfitted.\n",
        "\n",
        "We then increase the training epoch to 30, and the model performs similarly as before, with the BLEU score of the dev set close to saturation, and the difference between the loss in the training and dev sets still small, indicating that the model is close to convergence, but there is only limited room for further training to improve the model.\n",
        "\n",
        "However, when the epoch is increased to 50, the loss in the dev set slowly rises, and the BLEU score no longer gradually increases, but rather decreases, which indicates that the model is beginning to overfit, i.e., the model has learned too much detail and noise in the training set, but fails to effectively generalize to the dev set.\n",
        "\n",
        "Therefore, there are still some improvements we can make: introduce early stopping, add regularization (e.g., L2 regularization or Dropout), etc."
      ],
      "metadata": {
        "id": "4MeB7Ppzpzzb"
      }
    }
  ]
}